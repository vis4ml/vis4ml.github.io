const generatedBibEntries = {
    "ahn2019fairsight": {
        "abstract": "Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions - understanding, measuring, diagnosing and mitigating biases - that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.",
        "author": "Ahn, Yongsu and Lin, Yu-Ru",
        "doi": "10.1109/TVCG.2019.2934262",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "1086--1095",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Fairsight: Visual Analytics for Fairness in Decision Making",
        "type": "article",
        "url": "https://arxiv.org/pdf/1908.00176.pdf",
        "volume": "26",
        "year": "2019"
    },
    "bauerle2020classifier": {
        "abstract": "Training data plays an essential role in modern applications of machine learning. However, gathering labeled training data is time-consuming. Therefore, labeling is often outsourced to less experienced users, or completely automated. This can introduce errors, which compromise valuable training data, and lead to suboptimal training results. We thus propose a novel approach that uses the power of pretrained classifiers to visually guide users to noisy labels, and let them interactively check error candidates, to iteratively improve the training data set. To systematically investigate training data, we propose a categorization of labeling errors into three different types, based on an analysis of potential pitfalls in label acquisition processes. For each of these types, we present approaches to detect, reason about, and resolve error candidates, as we propose measures and visual guidance techniques to support machine learning users. Our approach has been used to spot errors in well-known machine learning benchmark data sets, and we tested its usability during a user evaluation. While initially developed for images, the techniques presented in this paper are independent of the classification algorithm, and can also be extended to many other types of training data.",
        "author": "B{\\\"a}uerle, Alex and Neumann, Heiko and Ropinski, Timo",
        "booktitle": "Computer Graphics Forum",
        "doi": "10.1111/cgf.13973",
        "keywords": "type:General ML,data:Sequence,task:Present,task:Explore,task:Assess,task:Improve",
        "number": "3",
        "pages": "195--205",
        "series": "EuroVis",
        "title": "Classifier-Guided Visual Correction of Noisy Labels for Image Classification Tasks",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13973",
        "volume": "39",
        "year": "2020"
    },
    "berger2020visually": {
        "abstract": "In this paper we introduce a method for visually analyzing contextualized embeddings produced by deep neural network-based language models. Our approach is inspired by linguistic probes for natural language processing, where tasks are designed to probe language models for linguistic structure, such as parts-of-speech and named entities. These approaches are largely confirmatory, however, only enabling a user to test for information known a priori. In this work, we eschew supervised probing tasks, and advocate for unsupervised probes, coupled with visual exploration techniques, to assess what is learned by language models. Specifically, we cluster contextualized embeddings produced from a large text corpus, and introduce a visualization design based on this clustering and textual structure - cluster co-occurrences, cluster spans, and cluster-word membership- to help elicit the functionality of, and relationship between, individual clusters. User feedback highlights the benefits of our design in discovering different types of linguistic structures.",
        "author": "Berger, Matthew",
        "booktitle": "2020 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VIS47514.2020.00062",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore",
        "pages": "276--280",
        "series": "Vis Short",
        "title": "Visually Analyzing Contextualized Embeddings",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/2009.02554.pdf",
        "year": "2020"
    },
    "bertucci2022dendromap": {
        "abstract": "In this paper, we present DendroMap, a novel approach to interactively exploring large-scale image datasets for machine learning (ML). ML practitioners often explore image datasets by generating a grid of images or projecting high-dimensional representations of images into 2-D using dimensionality reduction techniques (e.g., t-SNE). However, neither approach effectively scales to large datasets because images are ineffectively organized and interactions are insufficiently supported. To address these challenges, we develop DendroMap by adapting Treemaps, a well-known visualization technique. DendroMap effectively organizes images by extracting hierarchical cluster structures from high-dimensional representations of images. It enables users to make sense of the overall distributions of datasets and interactively zoom into specific areas of interests at multiple levels of abstraction. Our case studies with widely-used image datasets for deep learning demonstrate that users can discover insights about datasets and trained models by examining the diversity of images, identifying underperforming subgroups, and analyzing classification errors. We conducted a user study that evaluates the effectiveness of DendroMap in grouping and searching tasks by comparing it with a gridified version of t-SNE and found that participants preferred DendroMap. DendroMap is available at https://div-lab.github.io/dendromap/.",
        "author": "Bertucci, Donald and Hamid, Md Montaser and Anand, Yashwanthi and Ruangrotsakun, Anita and Tabatabai, Delyar and Perez, Melissa and Kahng, Minsuk",
        "doi": "10.1109/TVCG.2022.3209425",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "320--330",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "DendroMap: Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9904448",
        "volume": "29",
        "year": "2022"
    },
    "bilal2017convolutional": {
        "abstract": "Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help in identifying various quality issues in the training data.",
        "author": "Bilal, Alsallakh and Jourabloo, Amin and Ye, Mao and Liu, Xiaoming and Ren, Liu",
        "doi": "10.1109/TVCG.2017.2744683",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Improve",
        "number": "1",
        "pages": "152--162",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Do Convolutional Neural Networks Learn Class Hierarchy?",
        "type": "article",
        "url": "https://arxiv.org/pdf/1710.06501.pdf",
        "volume": "24",
        "year": "2017"
    },
    "bodria2022explaining": {
        "abstract": "Autoencoders are a powerful yet opaque feature reduction technique, on top of which we propose a novel way for the joint visual exploration of both latent and real space. By interactively exploiting the mapping between latent and real features, it is possible to unveil the meaning of latent features while providing deeper insight into the original variables. To achieve this goal, we exploit and re-adapt existing approaches from eXplainable Artificial Intelligence (XAI) to understand the relationships between the input and latent features. The uncovered relationships between input features and latent ones allow the user to understand the data structure concerning external variables such as the predictions of a classification model. We developed an interactive framework that visually explores the latent space and allows the user to understand the relationships of the input features with model prediction.",
        "author": "Bodria, Francesco and Rinzivillo, Salvatore and Fadda, Daniele and Guidotti, Riccardo and Giannotti, Fosca and Pedreschi, Dino",
        "booktitle": "EuroVis 2022 - Short Papers",
        "doi": "10.2312/evs.20221098",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Generate",
        "pages": "85--89",
        "publisher": "The Eurographics Association",
        "series": "EuroVis Short",
        "title": "Explaining Black Box with visual exploration of Latent Space",
        "type": "inproceedings",
        "url": "https://diglib.eg.org/handle/10.2312/evs20221098",
        "year": "2022"
    },
    "cabrera2019fairvis": {
        "abstract": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.",
        "author": "Cabrera, {\\'A}ngel Alexander and Epperson, Will and Hohman, Fred and Kahng, Minsuk and Morgenstern, Jamie and Chau, Duen Horng",
        "booktitle": "2019 IEEE Conference on Visual Analytics Science and Technology (VAST)",
        "doi": "10.1109/VAST47406.2019.8986948",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "organization": "IEEE",
        "pages": "46--56",
        "series": "VIS (VAST)",
        "title": "FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/1904.05419.pdf",
        "year": "2019"
    },
    "cao2020analyzing": {
        "abstract": "Adversarial examples, generated by adding small but intentionally imperceptible perturbations to normal examples, can mislead deep neural networks (DNNs) to make incorrect predictions. Although much work has been done on both adversarial attack and defense, a fine-grained understanding of adversarial examples is still lacking. To address this issue, we present a visual analysis method to explain why adversarial examples are misclassified. The key is to compare and analyze the datapaths of both the adversarial and normal examples. A datapath is a group of critical neurons along with their connections. We formulate the datapath extraction as a subset selection problem and solve it by constructing and training a neural network. A multi-level visualization consisting of a network-level visualization of data flows, a layer-level visualization of feature maps, and a neuron-level visualization of learned features, has been designed to help investigate how datapaths of adversarial and normal examples diverge and merge in the prediction process. A quantitative evaluation and a case study were conducted to demonstrate the promise of our method to explain the misclassification of adversarial examples.",
        "author": "Cao, Kelei and Liu, Mengchen and Su, Hang and Wu, Jing and Zhu, Jun and Liu, Shixia",
        "doi": "10.1109/TVCG.2020.2969185",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare",
        "number": "7",
        "pages": "3289-3304",
        "series": "TVCG",
        "title": "Analyzing the noise robustness of deep neural networks",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "27",
        "year": "2021"
    },
    "cashman2018rnnbow": {
        "abstract": "We present RNNbow, an interactive tool for visualizing the gradient flow during backpropagation in training of recurrent neural networks. By visualizing the gradient, as opposed to activations, RNNbow offers insight into how the network is learning. We show how it illustrates the vanishing gradient and the training process.",
        "author": "Cashman, Dylan and Patterson, Genevieve and Mosca, Abigail and Watts, Nathan and Robinson, Shannon and Chang, Remco",
        "doi": "10.1109/MCG.2018.2878902",
        "journal": "IEEE Computer Graphics and Applications",
        "keywords": "type:DL,data:Sequence,task:Present",
        "number": "6",
        "pages": "39--50",
        "publisher": "IEEE",
        "series": "CG\\&A",
        "title": "Rnnbow: Visualizing learning via backpropagation gradients in rnns",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/8617747",
        "volume": "38",
        "year": "2018"
    },
    "cavallo2018clustrophile": {
        "abstract": "Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2 , a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.",
        "author": "Cavallo, Marco and Demiralp, {\\c{C}}a{\\u{g}}atay",
        "doi": "10.1109/TVCG.2018.2864477",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "267--276",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Clustrophile 2: Guided visual clustering analysis",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/8440035",
        "volume": "25",
        "year": "2018"
    },
    "chatzimparmpas2020t": {
        "abstract": "t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of multidimensional data has proven to be a popular approach, with successful applications in a wide range of domains. Despite their usefulness, t-SNE projections can be hard to interpret or even misleading, which hurts the trustworthiness of the results. Understanding the details of t-SNE itself and the reasons behind specific patterns in its output may be a daunting task, especially for non-experts in dimensionality reduction. In this article, we present t-viSNE, an interactive tool for the visual exploration of t-SNE projections that enables analysts to inspect different aspects of their accuracy and meaning, such as the effects of hyper-parameters, distance and neighborhood preservation, densities and costs of specific neighborhoods, and the correlations between dimensions and visual patterns. We propose a coherent, accessible, and well-integrated collection of different views for the visualization of t-SNE projections. The applicability and usability of t-viSNE are demonstrated through hypothetical usage scenarios with real data sets. Finally, we present the results of a user study where the tool's effectiveness was evaluated. By bringing to light information that would normally be lost after running t-SNE, we hope to support analysts in using t-SNE and making its results better understandable.",
        "author": "Chatzimparmpas, Angelos and Martins, Rafael M and Kerren, Andreas",
        "doi": "10.1109/TVCG.2020.2986996",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "number": "8",
        "pages": "2696--2714",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "t-visne: Interactive assessment and interpretation of t-sne projections",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9064929",
        "volume": "26",
        "year": "2020"
    },
    "chatzimparmpas2022featureenvi": {
        "abstract": "The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data\u2014including complex feature engineering processes\u2014to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.",
        "author": "Chatzimparmpas, Angelos and Martins, Rafael M and Kucher, Kostiantyn and Kerren, Andreas",
        "doi": "10.1109/TVCG.2022.3141040",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Improve",
        "number": "4",
        "pages": "1773--1791",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Featureenvi: Visual analytics for feature engineering using stepwise selection and semi-automatic extraction approaches",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9672706",
        "volume": "28",
        "year": "2022"
    },
    "chen2017qsanglyzer": {
        "abstract": "Developing sophisticated artificial intelligence (AI) systems requires AI researchers to experiment with different designs and analyze results from evaluations (we refer this task as evaluation analysis). In this paper, we tackle the challenges of evaluation analysis in the domain of question-answering (QA) systems. Through in-depth studies with QA researchers, we identify tasks and goals of evaluation analysis and derive a set of design rationales, based on which we propose a novel approach termed prismatic analysis. Prismatic analysis examines data through multiple ways of categorization (referred as angles). Categories in each angle are measured by aggregate metrics to enable diverse comparison scenarios. To facilitate prismatic analysis of QA evaluations, we design and implement the Question Space Anglyzer (QSAnglyzer), a visual analytics (VA) tool. In QSAnglyzer, the high-dimensional space formed by questions is divided into categories based on several angles (e.g., topic and question type). Each category is aggregated by accuracy, the number of questions, and accuracy variance across evaluations. QSAnglyzer visualizes these angles so that QA researchers can examine and compare evaluations from various aspects both individually and collectively. Furthermore, QA researchers filter questions based on any angle by clicking to construct complex queries. We validate QSAnglyzer through controlled experiments and by expert reviews. The results indicate that when using QSAnglyzer, users perform analysis tasks faster (p <; 0.01) and more accurately (p <; 0.05), and are quick to gain new insight. We discuss how prismatic analysis and QSAnglyzer scaffold evaluation analysis, and provide directions for future research.",
        "author": "Chen, Nan-Chen and Kim, Been",
        "booktitle": "2017 IEEE Conference on Visual Analytics Science and Technology (VAST)",
        "doi": "10.1109/VAST.2017.8585733",
        "keywords": "type:General ML,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "organization": "IEEE",
        "pages": "48--58",
        "series": "VIS (VAST)",
        "title": "Qsanglyzer: Visual Analytics for Prismatic Analysis of Question Answering System Evaluations",
        "type": "inproceedings",
        "url": "http://www.nanro.org/publications/qsanglyzer\\_vast2017.pdf",
        "year": "2017"
    },
    "chen2020oodanalyzer": {
        "abstract": "One major cause of performance degradation in predictive models is that the test samples are not well covered by the training data. Such not well-represented samples are called OoD samples. In this article, we propose OoDAnalyzer, a visual analysis approach for interactively identifying OoD samples and explaining them in context. Our approach integrates an ensemble OoD detection method and a grid-based visualization. The detection method is improved from deep ensembles by combining more features with algorithms in the same family. To better analyze and understand the OoD samples in context, we have developed a novel k NN-based grid layout algorithm motivated by Hall's theorem. The algorithm approximates the optimal layout and has O(kN2) time complexity, faster than the grid layout algorithm with overall best performance but O(N3) time complexity. Quantitative evaluation and case studies were performed on several datasets to demonstrate the effectiveness and usefulness of OoDAnalyzer.",
        "author": "Chen, Changjian and Yuan, Jun and Lu, Yafeng and Liu, Yang and Su, Hang and Yuan, Songtao and Liu, Shixia",
        "doi": "10.1109/TVCG.2020.2973258",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Improve",
        "number": "7",
        "pages": "3335--3349",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Oodanalyzer: Interactive analysis of out-of-distribution samples",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp",
        "volume": "27",
        "year": "2021"
    },
    "chen2021interactive": {
        "abstract": "Semi-supervised learning (SSL) provides a way to improve the performance of prediction models (e.g., classifier) via the usage of unlabeled samples. An effective and widely used method is to construct a graph that describes the relationship between labeled and unlabeled samples. Practical experience indicates that graph quality significantly affects the model performance. In this paper, we present a visual analysis method that interactively constructs a high-quality graph for better model performance. In particular, we propose an interactive graph construction method based on the large margin principle. We have developed a river visualization and a hybrid visualization that combines a scatterplot, a node-link diagram, and a bar chart to convey the label propagation of graph-based SSL. Based on the understanding of the propagation, a user can select regions of interest to inspect and modify the graph. We conducted two case studies to showcase how our method facilitates the exploitation of labeled and unlabeled samples for improving model performance.",
        "author": "Chen, Changjian and Wang, Zhaowei and Wu, Jing and Wang, Xiting and Guo, Lan-Zhe and Li, Yu-Feng and Liu, Shixia",
        "doi": "10.1109/TVCG.2021.3084694",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Assess,task:Improve",
        "number": "9",
        "pages": "3701--3716",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Interactive graph construction for graph-based semi-supervised learning",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9444198",
        "volume": "27",
        "year": "2021"
    },
    "chen2021towards": {
        "abstract": "As training high-performance object detectors requires expensive bounding box annotations, recent methods resort to free-available image captions. However, detectors trained on caption supervision perform poorly because captions are usually noisy and cannot provide precise location information. To tackle this issue, we present a visual analysis method, which tightly integrates caption supervision with object detection to mutually enhance each other. In particular, object labels are first extracted from captions, which are utilized to train the detectors. Then, the objects detected from images are fed into caption supervision for further improvement. To effectively loop users into the object detection process, a node-link-based set visualization supported by a multi-type relational co-clustering algorithm is developed to explain the relationships between the extracted labels and the images with detected objects. The co-clustering algorithm clusters labels and images simultaneously by utilizing both their representations and their relationships. Quantitative evaluations and a case study are conducted to demonstrate the efficiency and effectiveness of the developed method in improving the performance of object detectors.",
        "author": "Chen, Changjian and Wu, Jing and Wang, Xiaohan and Xiang, Shouxing and Zhang, Song-Hai and Tang, Qifeng and Liu, Shixia",
        "doi": "10.1109/TVCG.2021.3138933",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Generate,task:Improve",
        "number": "4",
        "pages": "1941--1954",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Towards better caption supervision for object detection",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9664269",
        "volume": "28",
        "year": "2022"
    },
    "cheng2020dece": {
        "abstract": "With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable-a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.",
        "author": "Cheng, Furui and Ming, Yao and Qu, Huamin",
        "doi": "10.1109/TVCG.2020.3030342",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Generate",
        "number": "2",
        "pages": "1438--1447",
        "series": "VIS",
        "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "27",
        "year": "2020"
    },
    "cheng2021vbridge": {
        "abstract": "Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.",
        "author": "Cheng, Furui and Liu, Dongyu and Du, Fan and Lin, Yanna and Zytek, Alexandra and Li, Haomin and Qu, Huamin and Veeramachaneni, Kalyan",
        "doi": "10.1109/TVCG.2021.3114836",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare",
        "number": "1",
        "pages": "378--388",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Vbridge: Connecting the dots between features and data to explain healthcare models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9555810",
        "volume": "28",
        "year": "2021"
    },
    "collaris2020explainexplore": {
        "abstract": "Machine learning models often exhibit complex behavior that is difficult to understand. Recent research in explainable AI has produced promising techniques to explain the inner workings of such models using feature contribution vectors. These vectors are helpful in a wide variety of applications. However, there are many parameters involved in this process and determining which settings are best is difficult due to the subjective nature of evaluating interpretability. To this end, we introduce EXPLAINEXPLORE: an interactive explanation system to explore explanations that fit the subjective preference of data scientists. We leverage the domain knowledge of the data scientist to find optimal parameter settings and instance perturbations, and enable the discussion of the model and its explanation with domain experts. We present a use case on a real-world dataset to demonstrate the effectiveness of our approach for the exploration and tuning of machine learning explanations.",
        "author": "Collaris, Dennis and van Wijk, Jarke J",
        "booktitle": "2020 IEEE Pacific Visualization Symposium",
        "doi": "10.1109/PacificVis48177.2020.7090",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Generate",
        "organization": "IEEE",
        "pages": "26--35",
        "series": "PacificVis",
        "title": "ExplainExplore: Visual Exploration of Machine Learning Explanations",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9086281",
        "year": "2020"
    },
    "collaris2022strategyatlas": {
        "abstract": "Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.",
        "author": "Collaris, Dennis and Van Wijk, Jarke",
        "doi": "10.1109/TVCG.2022.3146806",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "6",
        "pages": "2996--2996",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "StrategyAtlas: Strategy Analysis for Machine Learning Interpretability",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9695246",
        "volume": "29",
        "year": "2023"
    },
    "das2019beames": {
        "abstract": "Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.",
        "author": "Das, Subhajit and Cashman, Dylan and Chang, Remco and Endert, Alex",
        "doi": "10.1109/MCG.2019.2922592",
        "journal": "IEEE Computer Graphics and Applications",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess,task:Improve",
        "number": "5",
        "pages": "20--32",
        "publisher": "IEEE",
        "series": "CG\\&A",
        "title": "Beames: Interactive Multi-Model Steering, Selection, and Inspection for Tegression Tasks",
        "type": "article",
        "url": "http://www.cs.tufts.edu/\\textasciitilde remco/publications/2019/CGA2019-Beames.pdf",
        "volume": "39",
        "year": "2019"
    },
    "das2020bluff": {
        "abstract": "Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model's internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.",
        "author": "Das, Nilaksh and Park, Haekyu and Wang, Zijie J and Hohman, Fred and Firstman, Robert and Rogers, Emily and Chau, Duen Horng Polo",
        "booktitle": "2020 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VIS47514.2020.00061",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare",
        "pages": "271--275",
        "series": "Vis Short",
        "title": "Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/2009.02608.pdf",
        "year": "2020"
    },
    "delaforge2022ebbe": {
        "abstract": "While neural networks (NN) have been successfully applied to many NLP tasks, the way they function is often difficult to interpret. In this article, we focus on binary text classification via NNs and propose a new tool, which includes a visualization of the decision boundary and the distances of data elements to this boundary. This tool increases the interpretability of NN. Our approach uses two innovative views: (1) an overview of the text representation space and (2) a local view allowing data exploration around the decision boundary for various localities of this representation space. These views are integrated into a visual platform, EBBE-Text, which also contains state-of-the-art visualizations of NN representation spaces and several kinds of information obtained from the classification process. The various views are linked through numerous interactive functionalities that enable easy exploration of texts and classification results via the various complementary views. A user study shows the effectiveness of the visual encoding and a case study illustrates the benefits of using our tool for the analysis of the classifications obtained with several recent NNs and two datasets.",
        "author": "Delaforge, Alexis and Az{\\'e}, J{\\'e}r{\\^o}me and Bringay, Sandra and Mollevi, Caroline and Sallaberry, Arnaud and Servajean, Maximilien",
        "doi": "10.1109/TVCG.2022.3184247",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "EBBE-Text: Explaining Neural Networks by Exploring Text Classification Decision Boundaries",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9801527",
        "year": "2023"
    },
    "derose2020attention": {
        "abstract": "Advances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.",
        "author": "DeRose, Joseph F and Wang, Jiayao and Berger, Matthew",
        "doi": "10.1109/TVCG.2020.3028976",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare",
        "number": "2",
        "pages": "1160--1170",
        "series": "VIS",
        "title": "Attention flows: Analyzing and comparing attention mechanisms in language models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp",
        "volume": "27",
        "year": "2020"
    },
    "dingen2018regressionexplorer": {
        "abstract": "We present RegressionExplorer, a Visual Analytics tool for the interactive exploration of logistic regression models. Our application domain is Clinical Biostatistics, where models are derived from patient data with the aim to obtain clinically meaningful insights and consequences. Development and interpretation of a proper model requires domain expertise and insight into model characteristics. Because of time constraints, often a limited number of candidate models is evaluated. RegressionExplorer enables experts to quickly generate, evaluate, and compare many different models, taking the workflow for model development as starting point. Global patterns in parameter values of candidate models can be explored effectively. In addition, experts are enabled to compare candidate models across multiple subpopulations. The insights obtained can be used to formulate new hypotheses or to steer model development. The effectiveness of the tool is demonstrated for two uses cases: prediction of a cardiac conduction disorder in patients after receiving a heart valve implant and prediction of hypernatremia in critically ill patients.",
        "author": "Dingen, Dennis and van't Veer, Marcel and Houthuizen, Patrick and Mestrom, Eveline HJ and Korsten, Erik HHM and Bouwman, Arthur RA and Van Wijk, Jarke",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "246--255",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "RegressionExplorer: Interactive exploration of logistic regression models with subgroup analysis",
        "type": "article",
        "volume": "25",
        "year": "2018"
    },
    "dong2020interactive": {
        "abstract": "Conventional attention visualization tools compromise either the readability or the information conveyed when documents are lengthy, especially when these documents have imbalanced sizes. Our work strives toward a more intuitive visualization for a subset of Natural Language Processing tasks, where attention is mapped between documents with imbalanced sizes. We extend the flow map visualization to enhance the readability of the attention-augmented documents. Through interaction, our design enables semantic filtering that helps users prioritize important tokens and meaningful matching for an in-depth exploration. Case studies and informal user studies in machine comprehension prove that our visualization effectively helps users gain initial understandings about what their models are {paying attention to.} We discuss how the work can be extended to other domains, as well as being plugged into more end-to-end systems for model error analysis.",
        "author": "Dong, Zhihang and Wu, Tongshuang and Song, Sicheng and Zhang, Mingrui",
        "booktitle": "2020 IEEE Pacific Visualization Symposium",
        "doi": "10.1109/PacificVis48177.2020.1031",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore",
        "organization": "IEEE",
        "pages": "46--50",
        "series": "PacificVis Notes",
        "title": "Interactive Attention Model Explorer for Natural Language Processing Tasks with Unbalanced Data Sizes",
        "type": "inproceedings",
        "url": "https://homes.cs.washington.edu/\\textasciitilde wtshuang/static/papers/2020-pvis-attention.pdf",
        "year": "2020"
    },
    "garcia2019v": {
        "abstract": "The usage of deep learning models for tagging input data has increased over the past years because of their accuracy and high\u2010performance. A successful application is to score sleep stages. In this scenario, models are trained to predict the sleep stages of individuals. Although their predictive accuracy is high, there are still mis classifications that prevent doctors from properly diagnosing sleep\u2010related disorders. This paper presents a system that allows users to explore the output of deep learning models in a real\u2010life scenario to spot and analyze faulty predictions. These can be corrected by users to generate a sequence of sleep stages to be examined by doctors. Our approach addresses a real\u2010life scenario with absence of ground truth. It differs from others in that our goal is not to improve the model itself, but to correct the predictions it provides. We demonstrate that our approach is effective in identifying faulty predictions and helping users to fix them in the proposed use case.",
        "author": "Garcia Caballero, Humberto S and Westenberg, Michel A and Gebre, Binyam and van Wijk, Jarke J",
        "booktitle": "Computer Graphics Forum",
        "doi": "https://doi.org/10.1111/cgf.13667",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Assess",
        "number": "3",
        "organization": "Wiley Online Library",
        "pages": "1--12",
        "series": "EuroVis",
        "title": "V-Awake: A Visual Analytics Approach for Correcting Sleep Predictions from Deep Learning Models",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13667",
        "volume": "38",
        "year": "2019"
    },
    "gehrmann2019visual": {
        "abstract": "Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.",
        "author": "Gehrmann, Sebastian and Strobelt, Hendrik and Kr{\\\"u}ger, Robert and Pfister, Hanspeter and Rush, Alexander M",
        "doi": "10.1109/TVCG.2019.2934595",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore",
        "number": "1",
        "pages": "884--894",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Visual Interaction with Deep Learning Models through Collaborative Semantic Inference",
        "type": "article",
        "url": "https://arxiv.org/pdf/1907.10739.pdf",
        "volume": "26",
        "year": "2019"
    },
    "ghai2022d": {
        "abstract": "With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.",
        "author": "Ghai, Bhavya and Mueller, Klaus",
        "doi": "10.1109/TVCG.2022.3209484",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "number": "1",
        "pages": "473--482",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9903601",
        "volume": "29",
        "year": "2022"
    },
    "gleicher2020boxer": {
        "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.",
        "author": "Gleicher, Michael and Barve, Aditya and Yu, Xinyi and Heimerl, Florian",
        "booktitle": "Computer Graphics Forum",
        "doi": "10.1111/cgf.13972",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "3",
        "pages": "181--193",
        "series": "EuroVis",
        "title": "Boxer: Interactive Comparison of Classifier results",
        "type": "article",
        "url": "https://graphics.cs.wisc.edu/Papers/2020/GBYH20/boxer\\_eurovis.pdf",
        "volume": "39",
        "year": "2020"
    },
    "gomez2021advice": {
        "abstract": "Rapid improvements in the performance of machine learning models have pushed them to the forefront of data-driven decision-making. Meanwhile, the increased integration of these models into various application domains has further highlighted the need for greater interpretability and transparency. To identify problems such as bias, overfitting, and incorrect correlations, data scientists require tools that explain the mechanisms with which these model decisions are made. In this paper we introduce AdViCE, a visual analytics tool that aims to guide users in black-box model debugging and validation. The solution rests on two main visual user interface innovations: (1) an interactive visualization design that enables the comparison of decisions on user-defined data subsets; (2) an algorithm and visual design to compute and visualize counterfactual explanations - explanations that depict model outcomes when data features are perturbed from their original values. We provide a demonstration of the tool through a use case that showcases the capabilities and potential limitations of the proposed approach.",
        "author": "Gomez, Oscar and Holter, Steffen and Yuan, Jun and Bertini, Enrico",
        "booktitle": "2021 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VIS49827.2021.9623271",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "organization": "IEEE",
        "pages": "31--35",
        "series": "VIS Short",
        "title": "AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9623271",
        "year": "2021"
    },
    "gou2020vatld": {
        "abstract": "Traffic light detection is crucial for environment perception and decision-making in autonomous driving. State-of-the-art detectors are built upon deep Convolutional Neural Networks (CNNs) and have exhibited promising performance. However, one looming concern with CNN based detectors is how to thoroughly evaluate the performance of accuracy and robustness before they can be deployed to autonomous vehicles. In this work, we propose a visual analytics system, VATLD, equipped with a disentangled representation learning and semantic adversarial learning, to assess, understand, and improve the accuracy and robustness of traffic light detectors in autonomous driving applications. The disentangled representation learning extracts data semantics to augment human cognition with human-friendly visual summarization, and the semantic adversarial learning efficiently exposes interpretable robustness risks and enables minimal human interaction for actionable insights. We also demonstrate the effectiveness of various performance improvement strategies derived from actionable insights with our visual analytics system, VATLD, and illustrate some practical implications for safety-critical applications in autonomous driving.",
        "author": "Gou, Liang and Zou, Lincan and Li, Nanxiang and Hofmann, Michael and Shekar, Arvind Kumar and Wendt, Axel and Ren, Liu",
        "doi": "10.1109/TVCG.2020.3030350",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "number": "2",
        "pages": "261--271",
        "series": "VIS",
        "title": "VATLD: a visual analytics system to assess, understand and improve traffic light detection",
        "type": "article",
        "url": "https://arxiv.org/pdf/2009.12975.pdf",
        "volume": "27",
        "year": "2020"
    },
    "he2020dynamicsexplorer": {
        "abstract": "Deep reinforcement learning (RL), where a policy represented by a deep neural network is trained, has shown some success in playing video games and chess. However, applying RL to real-world tasks like robot control is still challenging. Because generating a massive number of samples to train control policies using RL on real robots is very expensive, hence impractical, it is common to train in simulations, and then transfer to real environments. The trained policy, however, may fail in the real world because of the difference between the training and the real environments, especially the difference in dynamics. To diagnose the problems, it is crucial for experts to understand (1) how the trained policy behaves under different dynamics settings, (2) which part of the policy affects the behaviors the most when the dynamics setting changes, and (3) how to adjust the training procedure to make the policy robust.This paper presents DynamicsExplorer, a visual analytics tool to diagnose the trained policy on robot control tasks under different dynamics settings. DynamicsExplorer allows experts to overview the results of multiple tests with different dynamics-related parameter settings so experts can visually detect failures and analyze the sensitivity of different parameters. Experts can further examine the internal activations of the policy for selected tests and compare the activations between success and failure tests. Such comparisons help experts form hypotheses about the policy and allows them to verify the hypotheses via DynamicsExplorer. Multiple use cases are presented to demonstrate the utility of DynamicsExplorer.",
        "author": "He, Wenbin and Lee, Teng-Yok and van Baar, Jeroen and Wittenburg, Kent and Shen, Han-Wei",
        "booktitle": "2020 IEEE Pacific Visualization Symposium",
        "doi": "10.1109/PacificVis48177.2020.7127",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Compare",
        "organization": "IEEE",
        "pages": "36--45",
        "series": "PacificVis",
        "title": "DynamicsExplorer: Visual Analytics for Robot Control Tasks involving Dynamics and LSTM-based Control Policies",
        "type": "inproceedings",
        "url": "https://www.merl.com/publications/docs/TR2020-011.pdf",
        "year": "2020"
    },
    "he2021can": {
        "abstract": "Semantic segmentation is a critical component in autonomous driving and has to be thoroughly evaluated due to safety concerns. Deep neural network (DNN) based semantic segmentation models are widely used in autonomous driving. However, it is challenging to evaluate DNN-based models due to their black-box-like nature, and it is even more difficult to assess model performance for crucial objects, such as lost cargos and pedestrians, in autonomous driving applications. In this work, we propose VASS , a V isual A nalytics approach to diagnosing and improving the accuracy and robustness of S emantic S egmentation models, especially for critical objects moving in various driving scenes. The key component of our approach is a context-aware spatial representation learning that extracts important spatial information of objects, such as position, size, and aspect ratio, with respect to given scene contexts. Based on this spatial representation, we first use it to create visual summarization to analyze models' performance. We then use it to guide the generation of adversarial examples to evaluate models' spatial robustness and obtain actionable insights. We demonstrate the effectiveness of VASS via two case studies of lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative evaluation on the improvement of models' performance with actionable insights obtained from VASS.",
        "author": "He, Wenbin and Zou, Lincan and Shekar, Arvind Kumar and Gou, Liang and Ren, Liu",
        "doi": "10.1109/TVCG.2021.3114855",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "number": "1",
        "pages": "1040--1050",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Where Can We Help? A Visual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552909",
        "volume": "28",
        "year": "2021"
    },
    "heimerl2018interactive": {
        "abstract": "Word vector embeddings are an emerging tool for natural language processing. They have proven beneficial for a wide variety of language processing tasks. Their utility stems from the ability to encode word relationships within the vector space. Applications range from components in natural language processing systems to tools for linguistic analysis in the study of language and literature. In many of these applications, interpreting embeddings and understanding the encoded grammatical and semantic relations between words is useful, but challenging. Visualization can aid in such interpretation of embeddings. In this paper, we examine the role for visualization in working with word vector embeddings. We provide a literature survey to catalogue the range of tasks where the embeddings are employed across a broad range of applications. Based on this survey, we identify key tasks and their characteristics. Then, we present visual interactive designs that address many of these tasks. The designs integrate into an exploration and analysis environment for embeddings. Finally, we provide example use cases for them and discuss domain user feedback.",
        "author": "Heimerl, Florian and Gleicher, Michael",
        "booktitle": "Computer Graphics Forum",
        "doi": "https://doi.org/10.1111/cgf.13417",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare",
        "number": "3",
        "organization": "Wiley Online Library",
        "pages": "253--265",
        "series": "EuroVis",
        "title": "Interactive Analysis of Word Vector Embeddings",
        "type": "inproceedings",
        "url": "https://graphics.cs.wisc.edu/Papers/2018/HG18/embeddings\\_preprint.pdf",
        "volume": "37",
        "year": "2018"
    },
    "heimerl2020embcomp": {
        "abstract": "This article introduces embComp , a novel approach for comparing two embeddings that capture the similarity between objects, such as word and document embeddings. We survey scenarios where comparing these embedding spaces is useful. From those scenarios, we derive common tasks, introduce visual analysis methods that support these tasks, and combine them into a comprehensive system. One of embComp \u2019s central features are overview visualizations that are based on metrics for measuring differences in the local structure around objects. Summarizing these local metrics over the embeddings provides global overviews of similarities and differences. Detail views allow comparison of the local structure around selected objects and relating this local information to the global views. Integrating and connecting all of these components, embComp supports a range of analysis workflows that help understand similarities and differences between embedding spaces. We assess our approach by applying it in several use cases, including understanding corpora differences via word vector embeddings, and understanding algorithmic differences in generating embeddings.",
        "author": "Heimerl, Florian and Kralj, Christoph and M{\\\"o}ller, Torsten and Gleicher, Michael",
        "doi": "10.1109/TVCG.2020.3045918",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,data:Graph,task:Present,task:Explore,task:Compare",
        "number": "8",
        "pages": "2953--2969",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "embcomp: Visual interactive comparison of vector embeddings",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9301222",
        "volume": "28",
        "year": "2022"
    },
    "hinterreiter2020confusionflow": {
        "abstract": "Classifiers are among the most widely used supervised machine learning algorithms. Many classification models exist, and choosing the right one for a given task is difficult. During model selection and debugging, data scientists need to assess classifiers' performances, evaluate their learning behavior over time, and compare different models. Typically, this analysis is based on single-number performance measures such as accuracy. A more detailed evaluation of classifiers is possible by inspecting class errors. The confusion matrix is an established way for visualizing these class errors, but it was not designed with temporal or comparative analysis in mind. More generally, established performance analysis systems do not allow a combined temporal and comparative analysis of class-level information. To address this issue, we propose ConfusionFlow, an interactive, comparative visualization tool that combines the benefits of class confusion matrices with the visualization of performance characteristics over time. ConfusionFlow is model-agnostic and can be used to compare performances for different model types, model architectures, and/or training and test datasets. We demonstrate the usefulness of ConfusionFlow in a case study on instance selection strategies in active learning. We further assess the scalability of ConfusionFlow and present a use case in the context of neural network pruning.",
        "author": "Hinterreiter, Andreas and Ruch, Peter and Stitz, Holger and Ennemoser, Martin and Bernard, J{\\\"u}rgen and Strobelt, Hendrik and Streit, Marc",
        "doi": "10.1109/TVCG.2020.3012063",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "2",
        "pages": "1222--1236",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Confusionflow: A model-agnostic visualization for temporal analysis of classifier confusion",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9149790",
        "volume": "28",
        "year": "2022"
    },
    "hohman2019s": {
        "abstract": "Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.",
        "author": "Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng Polo",
        "doi": "10.1109/TVCG.2019.2934659",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare",
        "number": "1",
        "pages": "1096--1106",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations",
        "type": "article",
        "url": "https://arxiv.org/pdf/1904.02323.pdf",
        "volume": "26",
        "year": "2019"
    },
    "hohman2019telegam": {
        "abstract": "While machine learning (ML) continues to find success in solving previously-thought hard problems, interpreting and exploring ML models remains challenging. Recent work has shown that visualizations are a powerful tool to aid debugging, analyzing, and interpreting ML models. However, depending on the complexity of the model (e.g., number of features), interpreting these visualizations can be difficult and may require additional expertise. Alternatively, textual descriptions, or verbalizations, can be a simple, yet effective way to communicate or summarize key aspects about a model, such as the overall trend in a model's predictions or comparisons between pairs of data instances. With the potential benefits of visualizations and verbalizations in mind, we explore how the two can be combined to aid ML interpretability. Specifically, we present a prototype system, TeleGam, that demonstrates how visualizations and verbalizations can collectively support interactive exploration of ML models, for example, generalized additive models (GAMs). We describe TELEGAM's interface and underlying heuristics to generate the verbalizations. We conclude by discussing how TeleGam can serve as a platform to conduct future studies for understanding user expectations and designing novel interfaces for interpretable ML.",
        "author": "Hohman, Fred and Srinivasan, Arjun and Drucker, Steven M",
        "booktitle": "2019 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VISUAL.2019.8933695",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare",
        "organization": "IEEE",
        "pages": "151--155",
        "series": "VIS Short",
        "title": "TeleGam: Combining visualization and verbalization for interpretable machine learning",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/8933695",
        "year": "2019"
    },
    "hoque2021outcome": {
        "abstract": "The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this article, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.",
        "author": "Hoque, Md Naimul and Mueller, Klaus",
        "doi": "10.1109/TVCG.2021.3102051",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare",
        "number": "12",
        "pages": "4728--4740",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Outcome-explorer: A causality guided interactive visual interface for interpretable algorithmic decision making",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9507307",
        "volume": "28",
        "year": "2022"
    },
    "hoque2022visual": {
        "abstract": "Data-centric AI has emerged as a new research area to systematically engineer the data to land AI models for real-world applications. As a core method for data-centric AI, data programming helps experts inject domain knowledge into data and label data at scale using carefully designed labeling functions (e.g., heuristic rules, logistics) . Though data programming has shown great success in the NLP domain, it is challenging to program image data because of a) the challenge to describe images using visual vocabulary without human annotations and b) lacking efficient tools for data programming of images. We present Visual Concept Programming, a first-of-its-kind visual analytics approach of using visual concepts to program image data at scale while requiring a few human efforts. Our approach is built upon three unique components. It first uses a self-supervised learning approach to learn visual representation at the pixel level and extract a dictionary of visual concepts from images without using any human annotations. The visual concepts serve as building blocks of labeling functions for experts to inject their domain knowledge. We then design interactive visualizations to explore and understand visual concepts and compose labeling functions with concepts without writing code . Finally, with the composed labeling functions, users can label the image data at scale and use the labeled data to refine the pixel-wise visual representation and concept quality. We evaluate the learned pixel-wise visual representation for the downstream task of semantic segmentation to show the effectiveness and usefulness of our approach. In addition, we demonstrate how our approach tackles real-world problems of image retrieval for autonomous driving.",
        "author": "Hoque, Md Naimul and He, Wenbin and Shekar, Arvind Kumar and Gou, Liang and Ren, Liu",
        "doi": "10.1109/TVCG.2022.3209466",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Improve",
        "number": "1",
        "pages": "74--83",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence At Scale",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9904017",
        "volume": "29",
        "year": "2022"
    },
    "huang2020interactive": {
        "abstract": "Existing interactive visualization tools for deep learning are mostly applied to the training, debugging, and refinement of neural network models working on natural images. However, visual analytics tools are lacking for the specific application of x-ray image classification with multiple structural attributes. In this paper, we present an interactive system for domain scientists to visually study the multiple attributes learning models applied to x-ray scattering images. It allows domain scientists to interactively explore this important type of scientific images in embedded spaces that are defined on the model prediction output, the actual labels, and the discovered feature space of neural networks. Users are allowed to flexibly select instance images, their clusters, and compare them regarding the specified visual representation of attributes. The exploration is guided by the manifestation of model performance related to mutual relationships among attributes, which often affect the learning accuracy and effectiveness. The system thus supports domain scientists to improve the training dataset and model, find questionable attributes labels, and identify outlier images or spurious data clusters. Case studies and scientists feedback demonstrate its functionalities and usefulness.",
        "author": "Huang, Xinyi and Jamonnak, Suphanut and Zhao, Ye and Wang, Boyu and Hoai, Minh and Yager, Kevin and Xu, Wei",
        "doi": "10.1109/TVCG.2020.3030384",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "2",
        "pages": "1312--1321",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Interactive visual study of multiple attributes learning model of x-ray scattering images",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9240062",
        "volume": "27",
        "year": "2020"
    },
    "huang2021visual": {
        "abstract": "Layer-wise Relevance Propagation (LRP) is an emerging and widely-used method for interpreting the prediction results of convolutional neural networks (CNN). LRP developers often select and employ different relevance backpropagation rules and parameters, to compute relevance scores on input images. However, there exists no obvious solution to define a \u201cbest\u201d LRP model. A satisfied model is highly reliant on pertinent images and designers' goals. We develop a visual model designer, named as VisLRPDesigner, to overcome the challenges in the design and use of LRP models. Various LRP rules are unified into an integrated framework with an intuitive workflow of parameter setup. VisLRPDesigner thus allows users to interactively configure and compare LRP models. It also facilitates relevance-based visual analysis with two important functions: relevance-based pixel flipping and neuron ablation. Several use cases illustrate the benefits of VisLRPDesigner. The usability and limitation of the visual designer is evaluated by LRP users.",
        "author": "Huang, Xinyi and Jamonnak, Suphanut and Zhao, Ye and Wu, Tsung Heng and Xu, Wei",
        "booktitle": "Computer Graphics Forum",
        "doi": "doi.org/10.1111/cgf.14302",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare",
        "number": "3",
        "organization": "Wiley Online Library",
        "pages": "227--238",
        "series": "EuroVis",
        "title": "A Visual Designer of Layer-wise Relevance Propagation Models",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14302",
        "volume": "40",
        "year": "2021"
    },
    "huang2022conceptexplainer": {
        "abstract": "Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network's latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate CONCEPTEXPLAINER, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how CONCEPTEXPLAINER supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.",
        "author": "Huang, Jinbin and Mishra, Aditi and Kwon, Bum-Chul and Bryan, Chris",
        "doi": "10.1109/TVCG.2022.3209384",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "831--841",
        "series": "VIS",
        "title": "ConceptExplainer: Understanding the Mental Model of Deep Learning Algorithms via Interactive Concept-based Explanations",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/9903285",
        "volume": "29",
        "year": "2022"
    },
    "jaunet2020drlviz": {
        "abstract": "We present DRLViz, a visual analytics interface to interpret the internal memory of an agent (e.g. a robot) trained using deep reinforcement learning. This memory is composed of large temporal vectors updated when the agent moves in an environment and is not trivial to understand due to the number of dimensions, dependencies to past vectors, spatial/temporal correlations, and co-correlation between dimensions. It is often referred to as a black box as only inputs (images) and outputs (actions) are intelligible for humans. Using DRLViz, experts are assisted to interpret decisions using memory reduction interactions, and to investigate the role of parts of the memory when errors have been made (e.g. wrong direction). We report on DRLViz applied in the context of video games simulators (ViZDoom) for a navigation scenario with item gathering tasks. We also report on experts evaluation using DRLViz, and applicability of DRLViz to other scenarios and navigation problems beyond simulation games, as well as its contribution to black box models interpretability and explain-ability in the field of visual analytics.",
        "author": "Jaunet, Theo and Vuillemot, Romain and Wolf, Christian",
        "booktitle": "Computer Graphics Forum",
        "doi": "10.1111/cgf.13962",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Compare",
        "number": "3",
        "pages": "49--61",
        "series": "EuroVis",
        "title": "DRLViz: Understanding Decisions and Memory in Deep Reinforcement Learning",
        "type": "article",
        "url": "https://diglib.eg.org/bitstream/handle/10.1111/cgf13962/v39i3pp049-061.pdf",
        "volume": "39",
        "year": "2020"
    },
    "jaunet2021visqa": {
        "abstract": "Visual Question Answering systems target answering open-ended textual questions given input images. They are a testbed for learning high-level reasoning with a primary use in HCI, for instance assistance for the visually impaired. Recent research has shown that state-of-the-art models tend to produce answers exploiting biases and shortcuts in the training data, and sometimes do not even look at the input image, instead of performing the required reasoning steps. We present VisQA, a visual analytics tool that explores this question of reasoning vs. bias exploitation. It exposes the key element of state-of-the-art neural models \u2014 attention maps in transformers. Our working hypothesis is that reasoning steps leading to model predictions are observable from attention distributions, which are particularly useful for visualization. The design process of VisQA was motivated by well-known bias examples from the fields of deep learning and vision-language reasoning and evaluated in two ways. First, as a result of a collaboration of three fields, machine learning, vision and language reasoning, and data analytics, the work lead to a better understanding of bias exploitation of neural models for VQA, which eventually resulted in an impact on its design and training through the proposition of a method for the transfer of reasoning patterns from an oracle model. Second, we also report on the design of VisQA, and a goal-oriented evaluation of VisQA targeting the analysis of a model decision process from multiple experts, providing evidence that it makes the inner workings of models accessible to users.",
        "author": "Jaunet, Th{\\'e}o and Kervadec, Corentin and Vuillemot, Romain and Antipov, Grigory and Baccouche, Moez and Wolf, Christian",
        "doi": "10.1109/TVCG.2021.3114683",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "976--986",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552236",
        "volume": "28",
        "year": "2021"
    },
    "jeong2022interactively": {
        "abstract": "Generative adversarial networks (GAN) have witnessed tremendous growth in recent years, demonstrating wide applicability in many domains. However, GANs remain notoriously difficult for people to interpret, particularly for modern GANs capable of generating photo-realistic imagery. In this work we contribute a visual analytics approach for GAN interpretability, where we focus on the analysis and visualization of GAN disentanglement. Disentanglement is concerned with the ability to control content produced by a GAN along a small number of distinct, yet semantic, factors of variation. The goal of our approach is to shed insight on GAN disentanglement, above and beyond coarse summaries, instead permitting a deeper analysis of the data distribution modeled by a GAN. Our visualization allows one to assess a single factor of variation in terms of groupings and trends in the data distribution, where our analysis seeks to relate the learned representation space of GANs with attribute-based semantic scoring of images produced by GANs. Through use-cases, we show that our visualization is effective in assessing disentanglement, allowing one to quickly recognize a factor of variation and its overall quality. In addition, we show how our approach can highlight potential dataset biases learned by GANs.",
        "author": "Jeong, Sangwon and Liu, Shusen and Berger, Matthew",
        "booktitle": "Computer Graphics Forum",
        "doi": "doi.org/10.1111/cgf.14524",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "number": "3",
        "organization": "Wiley Online Library",
        "pages": "85--95",
        "series": "EuroVis",
        "title": "Interactively Assessing Disentanglement in GANs",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14524",
        "volume": "41",
        "year": "2022"
    },
    "ji2019visual": {
        "abstract": "Neural embeddings are widely used in language modeling and feature generation with superior computational power. Particularly, neural document embedding - converting texts of variable-length to semantic vector representations - has shown to benefit widespread downstream applications, e.g., information retrieval (IR). However, the black-box nature makes it difficult to understand how the semantics are encoded and employed. We propose visual exploration of neural document embedding to gain insights into the underlying embedding space, and promote the utilization in prevalent IR applications. In this study, we take an IR application-driven view, which is further motivated by biomedical IR in healthcare decision-making, and collaborate with domain experts to design and develop a visual analytics system. This system visualizes neural document embeddings as a configurable document map and enables guidance and reasoning; facilitates to explore the neural embedding space and identify salient neural dimensions (semantic features) per task and domain interest; and supports advisable feature selection (semantic analysis) along with instant visual feedback to promote IR performance. We demonstrate the usefulness and effectiveness of this system and present inspiring findings in use cases. This work will help designers/developers of downstream applications gain insights and confidence in neural document embedding, and exploit that to achieve more favorable performance in application domains.",
        "author": "Ji, Xiaonan and Shen, Han-Wei and Ritter, Alan and Machiraju, Raghu and Yen, Po-Yin",
        "doi": "10.1109/TVCG.2019.2903946",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore",
        "number": "6",
        "pages": "2181--2192",
        "publisher": "IEEE",
        "series": "PacificVis",
        "title": "Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection",
        "type": "article",
        "url": "https://www.researchgate.net/profile/Xiaonan\\_Ji2/publication/331797915\\_Visual\\_Exploration\\_of\\_Neural\\_Document\\_Embedding\\_in\\_Information\\_Retrieval\\_Semantics\\_and\\_Feature\\_Selection/links/5cb8cdf8a6fdcc1d499eee1a/Visual-Exploration-of-Neural-Document-Embedding-in-Information-Retrieval-Semantics-and-Feature-Selection.pdf",
        "volume": "25",
        "year": "2019"
    },
    "jia2021towards": {
        "abstract": "Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.",
        "author": "Jia, Shichao and Li, Zeyu and Chen, Nuo and Zhang, Jiawan",
        "doi": "10.1109/TVCG.2021.3114793",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Improve",
        "number": "1",
        "pages": "791--801",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Towards visual explainable active learning for zero-shot classification",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552842",
        "volume": "28",
        "year": "2021"
    },
    "jin2022gnnlens": {
        "abstract": "Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens, to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.",
        "author": "Jin, Zhihua and Wang, Yong and Wang, Qianwen and Ming, Yao and Ma, Tengfei and Qu, Huamin",
        "doi": "10.1109/TVCG.2022.3148107",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Graph,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "6",
        "pages": "3024--3038",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Gnnlens: A visual analytics approach for prediction error diagnosis of graph neural networks",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9705076",
        "volume": "29",
        "year": "2023"
    },
    "jin2022visual": {
        "abstract": "With deep learning (DL) outperforming conventional methods for different tasks, much effort has been devoted to utilizing DL in various domains. Researchers and developers in the traffic domain have also designed and improved DL models for forecasting tasks such as estimation of traffic speed and time of arrival. However, there exist many challenges in analyzing DL models due to the black-box property of DL models and complexity of traffic data (i.e., spatio-temporal dependencies). Collaborating with domain experts, we design a visual analytics system, AttnAnalyzer, that enables users to explore how DL models make predictions by allowing effective spatio-temporal dependency analysis. The system incorporates dynamic time warping (DTW) and Granger causality tests for computational spatio-temporal dependency analysis while providing map, table, line chart, and pixel views to assist user to perform dependency and model behavior analysis. For the evaluation, we present three case studies showing how AttnAnalyzer can effectively explore model behaviors and improve model performance in two different road networks. We also provide domain expert feedback.",
        "author": "Jin, Seungmin and Lee, Hyunwook and Park, Cheonbok and Chu, Hyeshin and Tae, Yunwon and Choo, Jaegul and Ko, Sungahn",
        "doi": "10.1109/TVCG.2022.3209462",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:DL,data:Sequence,data:Graph,data:Hybrid,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "1102--1112",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "A visual analytics system for improving attention-based traffic forecasting models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9903281",
        "volume": "29",
        "year": "2022"
    },
    "kahng2017cti": {
        "abstract": "While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.",
        "author": "Kahng, Minsuk and Andrews, Pierre Y and Kalro, Aditya and Chau, Duen Horng Polo",
        "doi": "10.1109/TVCG.2017.2744718",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "88--97",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models",
        "type": "article",
        "url": "https://arxiv.org/pdf/1704.01942.pdf",
        "volume": "24",
        "year": "2017"
    },
    "kahng2018gan": {
        "abstract": "Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.",
        "author": "Kahng, Minsuk and Thorat, Nikhil and Chau, Duen Horng Polo and Vi{\\'e}gas, Fernanda B and Wattenberg, Martin",
        "doi": "10.1109/TVCG.2018.2864500",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Tabular,task:Present,task:Explore",
        "number": "1",
        "pages": "1--11",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Gan Lab: Understanding Complex Deep Generative Models Using Interactive Visual Experimentation",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "25",
        "year": "2018"
    },
    "kaul2021improving": {
        "abstract": "Complex, high-dimensional data is used in a wide range of domains to explore problems and make decisions. Analysis of high-dimensional data, however, is vulnerable to the hidden influence of confounding variables, especially as users apply ad hoc filtering operations to visualize only specific subsets of an entire dataset. Thus, visual data-driven analysis can mislead users and encourage mistaken assumptions about causality or the strength of relationships between features. This work introduces a novel visual approach designed to reveal the presence of confounding variables via counterfactual possibilities during visual data analysis. It is implemented in CoFact, an interactive visualization prototype that determines and visualizes counterfactual subsets to better support user exploration of feature relationships. Using publicly available datasets, we conducted a controlled user study to demonstrate the effectiveness of our approach; the results indicate that users exposed to counterfactual visualizations formed more careful judgments about feature-to-outcome relationships.",
        "author": "Kaul, Smiti and Borland, David and Cao, Nan and Gotz, David",
        "doi": "10.1109/TVCG.2021.3114779",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "998--1008",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Improving Visualization Interpretation Using Counterfactuals",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552190",
        "volume": "28",
        "year": "2021"
    },
    "krause2017workflow": {
        "abstract": "Human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions. To this end, we propose a visual analytics workflow to help data scientists and domain experts explore, diagnose, and understand the decisions made by a binary classifier. The approach leverages \u201cinstance-level explanations\u201d, measures of local feature relevance that explain single instances, and uses them to build a set of visual representations that guide the users in their investigation. The workflow is based on three main visual representations and steps: one based on aggregate statistics to see how data distributes across correct / incorrect decisions; one based on explanations to understand which features are used to make these decisions; and one based on raw data, to derive insights on potential root causes for the observed patterns. The workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed. The case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes, thus experts can generate useful hypotheses on how a model can be improved.",
        "author": "Krause, Josua and Dasgupta, Aritra and Swartz, Jordan and Aphinyanaphongs, Yindalon and Bertini, Enrico",
        "booktitle": "2017 IEEE Conference on Visual Analytics Science and Technology (VAST)",
        "doi": "10.1109/VAST.2017.8585720",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "organization": "IEEE",
        "pages": "162--172",
        "series": "VIS (VAST)",
        "title": "A Workflow for Visual Diagnostics of Binary Classifiers Using Instance-Level Explanations",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/1705.01968.pdf",
        "year": "2017"
    },
    "kwon2017clustervision": {
        "abstract": "Clustering, the process of grouping together similar items into distinct partitions, is a common type of unsupervised machine learning that can be useful for summarizing and aggregating complex multi-dimensional data. However, data can be clustered in many ways, and there exist a large body of algorithms designed to reveal different patterns. While having access to a wide variety of algorithms is helpful, in practice, it is quite difficult for data scientists to choose and parameterize algorithms to get the clustering results relevant for their dataset and analytical tasks. To alleviate this problem, we built Clustervision, a visual analytics tool that helps ensure data scientists find the right clustering among the large amount of techniques and parameters available. Our system clusters data using a variety of clustering techniques and parameters and then ranks clustering results utilizing five quality metrics. In addition, users can guide the system to produce more relevant results by providing task-relevant constraints on the data. Our visual user interface allows users to find high quality clustering results, explore the clusters using several coordinated visualization techniques, and select the cluster result that best suits their task. We demonstrate this novel approach using a case study with a team of researchers in the medical domain and showcase that our system empowers users to choose an effective representation of their complex data.",
        "author": "Kwon, Bum Chul and Eysenbach, Ben and Verma, Janu and Ng, Kenney and De Filippi, Christopher and Stewart, Walter F and Perer, Adam",
        "doi": "10.1109/TVCG.2017.2745085",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "142--151",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Clustervision: Visual supervision of unsupervised clustering",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/8019866",
        "volume": "24",
        "year": "2017"
    },
    "kwon2018retainvis": {
        "abstract": "We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.",
        "author": "Kwon, Bum Chul and Choi, Min-Je and Kim, Joanne Taery and Choi, Edward and Kim, Young Bin and Kwon, Soonwook and Sun, Jimeng and Choo, Jaegul",
        "doi": "10.1109/TVCG.2018.2865027",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "299--309",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Retainvis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records",
        "type": "article",
        "url": "https://arxiv.org/pdf/1805.10724.pdf",
        "volume": "25",
        "year": "2018"
    },
    "kwon2022dash": {
        "abstract": "Image classification models often learn to predict a class based on irrelevant co-occurrences between input features and an output class in training data. We call the unwanted correlations ''data biases,'' and the visual features causing data biases ''bias factors.'' It is challenging to identify and mitigate biases automatically without human intervention. Therefore, we conducted a design study to find a human-in-the-loop solution. First, we identified user tasks that capture the bias mitigation process for image classification models with three experts. Then, to support the tasks, we developed a visual analytics system called DASH that allows users to visually identify bias factors, to iteratively generate synthetic images using a state-of-the-art image-toimage translation model, and to supervise the model training process for improving the classification accuracy. Our quantitative evaluation and qualitative study with ten participants demonstrate the usefulness of DASH and provide lessons for future work.",
        "author": "Kwon, Bum Chul and Lee, Jungsoo and Chung, Chaeyeon and Lee, Nyoungwoo and Choi, Ho-Jin and Choo, Jaegul",
        "booktitle": "EuroVis 2022 - Short Papers",
        "doi": "10.2312/evs.20221099",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Assess,task:Generate",
        "pages": "91--95",
        "publisher": "The Eurographics Association",
        "series": "EuroVis Short",
        "title": "DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation",
        "type": "inproceedings",
        "url": "https://diglib.eg.org/handle/10.2312/evs20221099",
        "year": "2022"
    },
    "kwon2022rmexplorer": {
        "abstract": "Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models developed on one dataset may not generalize across diverse subpopulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Specifically, the system allows users to define subgroups of patients by selecting clinical, demographic, or other characteristics, to explore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial fibrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.",
        "author": "Kwon, Bum Chul and Kartoun, Uri and Khurshid, Shaan and Yurochkin, Mikhail and Maity, Subha and Brockman, Deanna G and Khera, Amit V and Ellinor, Patrick T and Lubitz, Steven A and Ng, Kenney",
        "booktitle": "2022 IEEE Visualization and Visual Analytics (VIS)",
        "doi": "10.1109/VIS54862.2022.00019",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "organization": "IEEE",
        "pages": "50--54",
        "series": "VIS Short",
        "title": "RMExplorer: A Visual Analytics Approach to Explore the Performance and the Fairness of Disease Risk Models on Population Subgroups",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9973226",
        "year": "2022"
    },
    "lee2022visualization": {
        "abstract": "Given images of a person, person re- identification (Person ReID) techniques aim to find images of the same person from previously collected images. Because of large data sets of person images and the advance of deep learning, convolutional neural networks (CNNs) successfully boost the accuracy of Person ReID algorithms, but it can be difficult to explain and to troubleshoot issues due to the complexity of CNNs. In this paper, we present a visualization-based approach to understand a CNN-based Person ReID algorithm. As Person ReID algorithms are often designed to map images of the same person into similar feature vectors, given two images, we design an algorithm to estimate how much each element in a CNN layer contributes to the similarity between their feature vectors. Based on the estimation, we build a visualization tool to interactively locate and visualize the activation of highly-contributing elements, other than manually examining all. Our visualization tool also supports various user interaction widgets to explore a Person ReID data set, locate difficult cases, and analyze the reason behind their similarities. We show a use case with our tool to understand and troubleshoot issues in a CNN-based Person ReID algorithm.",
        "author": "Lee, Teng-Yok",
        "booktitle": "2022 IEEE 15th Pacific Visualization Symposium (PacificVis)",
        "doi": "10.1109/PacificVis53943.2022.00027",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare",
        "organization": "IEEE",
        "pages": "171--175",
        "series": "PacificVis Notes",
        "title": "Visualization for neural-network-based person re-identification",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9787825",
        "year": "2022"
    },
    "li2018embeddingvis": {
        "abstract": "Constructing latent vector representation for nodes in a network through embedding models has shown its practicality in many graph analysis applications, such as node classification, clustering, and link prediction. However, despite the high efficiency and accuracy of learning an embedding model, people have little clue of what information about the original network is preserved in the embedding vectors. The abstractness of low-dimensional vector representation, stochastic nature of the construction process, and non-transparent hyper-parameters all obscure understanding of network embedding results. Visualization techniques have been introduced to facilitate embedding vector inspection, usually by projecting the embedding space to a two-dimensional display. Although the existing visualization methods allow simple examination of the structure of embedding space, they cannot support in-depth exploration of the embedding vectors. In this paper, we design an exploratory visual analytics system that supports the comparative visual interpretation of embedding vectors at the cluster, instance, and structural levels. To be more specific, it facilitates comparison of what and how node metrics are preserved across different embedding models and investigation of relationships between node metrics and selected embedding vectors. Several case studies confirm the efficacy of our system. Experts' feedback suggests that our approach indeed helps them better embrace the understanding of network embedding models.",
        "author": "Li, Quan and Njotoprawiro, Kristanto Sean and Haleem, Hammad and Chen, Qiaoan and Yi, Chris and Ma, Xiaojuan",
        "booktitle": "2018 IEEE Conference on Visual Analytics Science and Technology (VAST)",
        "doi": "10.1109/VAST.2018.8802454",
        "keywords": "type:DL,data:Graph,task:Present,task:Explore,task:Compare",
        "organization": "IEEE",
        "pages": "48--59",
        "series": "VIS (VAST)",
        "title": "EmbeddingVis: A Visual Analytics Approach to Comparative Network Embedding Inspection",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/1808.09074.pdf",
        "year": "2018"
    },
    "li2020cnnpruner": {
        "abstract": "Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the pruning criteria, lacking the flexibility to optimally balance the trade-off between efficiency and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.",
        "author": "Li, Guan and Wang, Junpeng and Shen, Han-Wei and Chen, Kaixin and Shan, Guihua and Lu, Zhonghua",
        "doi": "10.1109/TVCG.2020.3030461",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Assess",
        "number": "2",
        "pages": "1364--1373",
        "series": "VIS",
        "title": "Cnnpruner: Pruning convolutional neural networks with visual analytics",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "27",
        "year": "2020"
    },
    "li2021inspecting": {
        "abstract": "As a decentralized training approach, horizontal federated learning (HFL) enables distributed clients to collaboratively learn a machine learning model while keeping personal/private information on local devices. Despite the enhanced performance and efficiency of HFL over local training, clues for inspecting the behaviors of the participating clients and the federated model are usually lacking due to the privacy-preserving nature of HFL. Consequently, the users can only conduct a shallow-level analysis of potential abnormal behaviors and have limited means to assess the contributions of individual clients and implement the necessary intervention. Visualization techniques have been introduced to facilitate the HFL process inspection, usually by providing model metrics and evaluation results as a dashboard representation. Although the existing visualization methods allow a simple examination of the HFL model performance, they cannot support the intensive exploration of the HFL process. In this article, strictly following the HFL privacy-preserving protocol, we design an exploratory visual analytics system for the HFL process termed HFLens , which supports comparative visual interpretation at the overview, communication round, and client instance levels. Specifically, the proposed system facilitates the investigation of the overall process involving all clients, the correlation analysis of clients\u2019 information in one or different communication round(s), the identification of potential anomalies, and the contribution assessment of each HFL client. Two case studies confirm the efficacy of our system. Experts\u2019 feedback suggests that our approach indeed helps in understanding and diagnosing the HFL process better.",
        "author": "Li, Quan and Wei, Xiguang and Lin, Huanbin and Liu, Yang and Chen, Tianjian and Ma, Xiaojuan",
        "doi": "10.1109/TVCG.2021.3074010",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "12",
        "pages": "4085--4100",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Inspecting the running process of horizontal federated learning via visual analytics",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/9408377",
        "volume": "28",
        "year": "2022"
    },
    "li2022unified": {
        "abstract": "The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.",
        "author": "Li, Zhen and Wang, Xiting and Yang, Weikai and Wu, Jing and Zhang, Zhengyan and Liu, Zhiyuan and Sun, Maosong and Zhang, Hui and Liu, Shixia",
        "doi": "10.1109/TVCG.2022.3184186",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "12",
        "pages": "4980--4994",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "A unified understanding of deep nlp models for text classification",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9801603",
        "volume": "28",
        "year": "2022"
    },
    "liu2016towards": {
        "abstract": "Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.",
        "author": "Liu, Mengchen and Shi, Jiaxin and Li, Zhen and Li, Chongxuan and Zhu, Jun and Liu, Shixia",
        "doi": "10.1109/TVCG.2016.2598831",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "91--100",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Towards Better Analysis of Deep Convolutional Neural Networks",
        "type": "article",
        "url": "https://arxiv.org/pdf/1604.07043.pdf",
        "volume": "23",
        "year": "2016"
    },
    "liu2017analyzing": {
        "abstract": "Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.",
        "author": "Liu, Mengchen and Shi, Jiaxin and Cao, Kelei and Zhu, Jun and Liu, Shixia",
        "doi": "10.1109/TVCG.2017.2744938",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Improve",
        "number": "1",
        "pages": "77--87",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Analyzing the Training Processes of Deep Generative Models",
        "type": "article",
        "url": "https://ml.cs.tsinghua.edu.cn/\\textasciitilde jun/pub/training-dgm.pdf",
        "volume": "24",
        "year": "2017"
    },
    "liu2017visual": {
        "abstract": "Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP). However, despite their widespread application, little is known about the structure and properties of these spaces. To gain insights into the relationship between words, the NLP community has begun to adapt high-dimensional visualization techniques. In particular, researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (e.g., word analogies), respectively. Unfortunately, these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings. Here, we introduce new embedding techniques for visualizing semantic and syntactic analogies, and the corresponding tests to determine whether the resulting views capture salient structures. Additionally, we introduce two novel views for a comprehensive study of analogy relationships. Finally, we augment t-SNE embeddings to convey uncertainty information in order to allow a reliable interpretation. Combined, the different views address a number of domain-specific tasks difficult to solve with existing tools.",
        "author": "Liu, Shusen and Bremer, Peer-Timo and Thiagarajan, Jayaraman J and Srikumar, Vivek and Wang, Bei and Livnat, Yarden and Pascucci, Valerio",
        "doi": "10.1109/TVCG.2017.2745141",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare",
        "number": "1",
        "pages": "553--562",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Visual Exploration of Semantic Relationships in Neural Word Embeddings",
        "type": "article",
        "url": "http://www.sci.utah.edu/\\textasciitilde shusenl/publications/paper157.pdf",
        "volume": "24",
        "year": "2017"
    },
    "liu2017visual1": {
        "abstract": "Tree boosting, which combines weak learners (typically decision trees) to generate a strong learner, is a highly effective and widely used machine learning method. However, the development of a high performance tree boosting model is a time-consuming process that requires numerous trial-and-error experiments. To tackle this issue, we have developed a visual diagnosis tool, BOOSTVis, to help experts quickly analyze and diagnose the training process of tree boosting. In particular, we have designed a temporal confusion matrix visualization, and combined it with a t-SNE projection and a tree visualization. These visualization components work together to provide a comprehensive overview of a tree boosting model, and enable an effective diagnosis of an unsatisfactory training process. Two case studies that were conducted on the Otto Group Product Classification Challenge dataset demonstrate that BOOSTVis can provide informative feedback and guidance to improve understanding and diagnosis of tree boosting algorithms.",
        "author": "Liu, Shixia and Xiao, Jiannan and Liu, Junlin and Wang, Xiting and Wu, Jing and Zhu, Jun",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "163--173",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Visual diagnosis of tree boosting methods",
        "type": "article",
        "volume": "24",
        "year": "2017"
    },
    "liu2018nlize": {
        "abstract": "With the recent advances in deep learning, neural network models have obtained state-of-the-art performances for many linguistic tasks in natural language processing. However, this rapid progress also brings enormous challenges. The opaque nature of a neural network model leads to hard-to-debug-systems and difficult-to-interpret mechanisms. Here, we introduce a visualization system that, through a tight yet flexible integration between visualization elements and the underlying model, allows a user to interrogate the model by perturbing the input, internal state, and prediction while observing changes in other parts of the pipeline. We use the natural language inference problem as an example to illustrate how a perturbation-driven paradigm can help domain experts assess the potential limitation of a model, probe its inner states, and interpret and form hypotheses about fundamental model mechanisms such as attention.",
        "author": "Liu, Shusen and Li, Zhimin and Li, Tao and Srikumar, Vivek and Pascucci, Valerio and Bremer, Peer-Timo",
        "doi": "10.1109/TVCG.2018.2865230",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "number": "1",
        "pages": "651--660",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "NLIZE: A Perturbation-Driven Visual Interrogation Tool for Analyzing and Interpreting Natural Language Inference Models",
        "type": "article",
        "url": "https://www.osti.gov/pages/servlets/purl/1562803",
        "volume": "25",
        "year": "2018"
    },
    "liu2019latent": {
        "abstract": "Latent spaces\u2014reduced-dimensionality vector space embeddings of data, fit via machine learning\u2014have been shown to capture interesting semantic properties and support data analysis and synthesis within a domain. Interpretation of latent spaces is challenging because prior knowledge, sometimes subtle and implicit, is essential to the process. We contribute methods for \u201clatent space cartography\u201d, the process of mapping and comparing meaningful semantic dimensions within latent spaces. We first perform a literature survey of relevant machine learning, natural language processing, and scientific research to distill common tasks and propose a workflow process. Next, we present an integrated visual analysis system for supporting this workflow, enabling users to discover, define, and verify meaningful relationships among data points, encoded within latent space dimensions. Three case studies demonstrate how users of our system can compare latent space variants in image generation, challenge existing findings on cancer transcriptomes, and assess a word embedding benchmark.",
        "author": "Liu, Yang and Jun, Eunice and Li, Qisheng and Heer, Jeffrey",
        "booktitle": "Computer Graphics Forum",
        "doi": "https://doi.org/10.1111/cgf.13672",
        "keywords": "type:DL,data:Sequence,data:MD-Array,task:Present,task:Explore,task:Compare",
        "number": "3",
        "organization": "Wiley Online Library",
        "pages": "67--78",
        "series": "EuroVis",
        "title": "Latent Space Cartography: Visual Analysis of Vector Space Embeddings",
        "type": "inproceedings",
        "url": "https://yangliu.life/build/misc/lsc.pdf",
        "volume": "38",
        "year": "2019"
    },
    "liu2022visualizing": {
        "abstract": "Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts. Availability: Open-source code at https://github.com/zipengliu/corgie-ui/ , supplemental materials \\& video at https://osf.io/tr3sb/.",
        "author": "Liu, Zipeng and Wang, Yang and Bernard, J{\\\"u}rgen and Munzner, Tamara",
        "doi": "10.1109/TVCG.2022.3148197",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Graph,task:Present,task:Explore,task:Compare",
        "number": "6",
        "pages": "2500--2516",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9705082",
        "volume": "28",
        "year": "2022"
    },
    "ma2019explaining": {
        "abstract": "Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.",
        "author": "Ma, Yuxin and Xie, Tiankai and Li, Jundong and Maciejewski, Ross",
        "doi": "10.1109/TVCG.2019.2934631",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "1075--1085",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics",
        "type": "article",
        "url": "https://arxiv.org/pdf/1907.07296.pdf",
        "volume": "26",
        "year": "2019"
    },
    "ma2020visual": {
        "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.",
        "author": "Ma, Yuxin and Fan, Arlen and He, Jingrui and Nelakurthi, Arun Reddy and Maciejewski, Ross",
        "doi": "10.1109/TVCG.2020.3028888",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "2",
        "pages": "1385--1395",
        "series": "VIS",
        "title": "A visual analytics framework for explaining and diagnosing transfer learning processes",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "27",
        "year": "2020"
    },
    "ma2020visual1": {
        "abstract": "High-dimensional labeled data widely exists in many real-world applications such as classification and clustering. One main task in analyzing such datasets is to explore class separations and class boundaries derived from machine learning models. Dimension reduction techniques are commonly applied to support analysts in exploring the underlying decision boundary structures by depicting a low-dimensional representation of the data distributions from multiple classes. However, such projection-based analyses are limited due to their lack of ability to show separations in complex non-linear decision boundary structures and can suffer from heavy distortion and low interpretability. To overcome these issues of separability and interpretability, we propose a visual analysis approach that utilizes the power of explainability from linear projections to support analysts when exploring non-linear separation structures. Our approach is to extract a set of locally linear segments that approximate the original non-linear separations. Unlike traditional projection-based analysis where the data instances are mapped to a single scatterplot, our approach supports the exploration of complex class separations through multiple local projection results. We conduct case studies on two labeled datasets to demonstrate the effectiveness of our approach.",
        "author": "Ma, Yuxin and Maciejewski, Ross",
        "doi": "10.1109/TVCG.2020.3011155",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "241--253",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Visual analysis of class separations with locally linear segments",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9146191",
        "volume": "27",
        "year": "2021"
    },
    "meng2022modelwise": {
        "abstract": "Model comparison is an important process to facilitate model diagnosis, improvement, and selection when multiple models are developed for a classification task. It involves careful comparison concerning model performance and interpretation. Current visual analytics solutions often ignore the feature selection process. They either do not support detailed analysis of multiple multi-class classifiers or rely on feature analysis alone to interpret model results. Understanding how different models make classification decisions, especially classification disagreements of the same instances, requires a deeper model understanding. We present ModelWise, a visual analytics method to compare multiple multi-class classifiers in terms of model performance, feature space, and model explanation. ModelWise adapts visualizations with rich interactions to support multiple workflows to achieve model diagnosis, improvement, and selection. It considers feature subspaces generated for use in different models and improves model understanding by model explanation. We demonstrate the usability of ModelWise with two case studies, one with a small exemplar dataset and another developed with a machine learning expert with real-world perioperative data.",
        "author": "Meng, Linhao and Van Den Elzen, Stef and Vilanova, Anna",
        "booktitle": "Computer Graphics Forum",
        "doi": "doi.org/10.1111/cgf.14525",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "3",
        "organization": "Wiley Online Library",
        "pages": "97--108",
        "series": "EuroVis",
        "title": "ModelWise: Interactive Model Comparison for Model Diagnosis, Improvement and Selection",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14525",
        "volume": "41",
        "year": "2022"
    },
    "ming2017understanding": {
        "abstract": "Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.",
        "author": "Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin",
        "booktitle": "2017 IEEE Conference on Visual Analytics Science and Technology (VAST)",
        "doi": "10.1109/VAST.2017.8585721",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare",
        "organization": "IEEE",
        "pages": "13--24",
        "series": "VIS (VAST)",
        "title": "Understanding Hidden Memories of Recurrent Neural Networks",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/1710.10777.pdf",
        "year": "2017"
    },
    "ming2018rulematrix": {
        "abstract": "With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.",
        "author": "Ming, Yao and Qu, Huamin and Bertini, Enrico",
        "doi": "10.1109/TVCG.2018.2864812",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "342--352",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "RuleMatrix: Visualizing and Understanding Classifiers with Rules",
        "type": "article",
        "url": "https://arxiv.org/pdf/1807.06228.pdf",
        "volume": "25",
        "year": "2018"
    },
    "ming2019protosteer": {
        "abstract": "Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.",
        "author": "Ming, Yao and Xu, Panpan and Cheng, Furui and Qu, Huamin and Ren, Liu",
        "doi": "10.1109/TVCG.2019.2934267",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Improve",
        "number": "1",
        "pages": "238--248",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "ProtoSteer: Steering Deep Sequence Model with Prototypes",
        "type": "article",
        "url": "https://lliquid.github.io/homepage/files/VIS2019\\_ProtoSteer.pdf",
        "volume": "26",
        "year": "2019"
    },
    "mishra2022not": {
        "abstract": "Reinforcement learning (RL) is used in many domains, including autonomous driving, robotics, stock trading, and video games. Unfortunately, the black box nature of RL agents, combined with legal and ethical considerations, makes it increasingly important that humans (including those are who not experts in RL) understand the reasoning behind the actions taken by an RL agent, particularly in safety-critical domains. To help address this challenge, we introduce PolicyExplainer, a visual analytics interface which lets the user directly query an autonomous agent. PolicyExplainer visualizes the states, policy, and expected future rewards for an agent, and supports asking and answering questions such as: \u201cWhy take this action? Why not take this other action? When is this action taken?\u201d PolicyExplainer is designed based upon a domain analysis with RL researchers, and is evaluated via qualitative and quantitative assessments on a trio of domains: taxi navigation, a stack bot domain, and drug recommendation for HIV patients. We find that PolicyExplainer's visual approach promotes trust and understanding of agent decisions better than a state-of-the-art text-based explanation approach. Interviews with domain practitioners provide further validation for PolicyExplainer as applied to safety-critical domains. Our results help demonstrate how visualization-based approaches can be leveraged to decode the behavior of autonomous RL agents, particularly for RL non-experts.",
        "author": "Mishra, Aditi and Soni, Utkarsh and Huang, Jinbin and Bryan, Chris",
        "booktitle": "2022 IEEE 15th Pacific Visualization Symposium (PacificVis)",
        "doi": "10.1109/PacificVis53943.2022.00020",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Assess",
        "organization": "IEEE",
        "pages": "111--120",
        "series": "PacificVis",
        "title": "Why? Why not? When? Visual Explanations of Agent Behaviour in Reinforcement Learning",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9787897",
        "year": "2022"
    },
    "munechika2022visual": {
        "abstract": "As machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their de-ployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underper-forming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overper-forming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.",
        "author": "Munechika, David and Wang, Zijie J and Reidy, Jack and Rubin, Josh and Gade, Krishna and Kenthapadi, Krishnaram and Chau, Duen Horng",
        "booktitle": "2022 IEEE Visualization and Visual Analytics (VIS)",
        "doi": "10.1109/VIS54862.2022.00018",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Assess",
        "organization": "IEEE",
        "pages": "45--49",
        "series": "VIS Short",
        "title": "Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9973204",
        "year": "2022"
    },
    "murugesan2019deepcompare": {
        "abstract": "Deep learning models have become the state-of-the-art for many tasks, from text sentiment analysis to facial image recognition. However, understanding why certain models perform better than others or how one model learns differently than another is often difficult yet critical for increasing their effectiveness, improving prediction accuracy, and enabling fairness. Traditional methods for comparing models' efficacy, such as accuracy, precision, and recall provide a quantitative view of performance; however, the qualitative intricacies of why one model performs better than another are hidden. In this paper, we interview machine learning practitioners to understand their evaluation and comparison workflow. From there, we iteratively design a visual analytic approach, DeepCompare, to systematically compare the results of deep learning models, in order to provide insight into the model behavior and interactively assess tradeoffs between two such models. The tool allows users to evaluate model results, identify and compare activation patterns for misclassifications, and link the test results back to specific neurons. We conduct a preliminary evaluation through two real-world case studies to show that experts can make more informed decisions about the effectiveness of different types of models, understand in more detail the strengths and weaknesses of the models, and holistically evaluate the behavior of the models.",
        "author": "Murugesan, Sugeerth and Malik, Sana and Du, Fan and Koh, Eunyee and Lai, Tuan Manh",
        "doi": "10.1109/MCG.2019.2919033",
        "journal": "IEEE computer graphics and applications",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "5",
        "pages": "47--59",
        "publisher": "IEEE",
        "series": "CG\\&A",
        "title": "Deepcompare: Visual and interactive comparison of deep learning model performance",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "39",
        "year": "2019"
    },
    "neto2020explainable": {
        "abstract": "Over the past decades, classification models have proven to be essential machine learning tools given their potential and applicability in various domains. In these years, the north of the majority of the researchers had been to improve quantitative metrics, notwithstanding the lack of information about models' decisions such metrics convey. This paradigm has recently shifted, and strategies beyond tables and numbers to assist in interpreting models' decisions are increasing in importance. Part of this trend, visualization techniques have been extensively used to support classification models' interpretability, with a significant focus on rule-based models. Despite the advances, the existing approaches present limitations in terms of visual scalability, and the visualization of large and complex models, such as the ones produced by the Random Forest (RF) technique, remains a challenge. In this paper, we propose Explainable Matrix (ExMatrix), a novel visualization method for RF interpretability that can handle models with massive quantities of rules. It employs a simple yet powerful matrix-like visual metaphor, where rows are rules, columns are features, and cells are rules predicates, enabling the analysis of entire models and auditing classification results. ExMatrix applicability is confirmed via different examples, showing how it can be used in practice to promote RF models interpretability.",
        "author": "Neto, M{\\'a}rio Popolin and Paulovich, Fernando V",
        "doi": "10.1109/TVCG.2020.3030354",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare",
        "number": "2",
        "pages": "1427--1437",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Explainable matrix-visualization for global and local interpretability of random forest classification ensembles",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9222255",
        "volume": "27",
        "year": "2020"
    },
    "nie2018visualizing": {
        "abstract": "Deep neural networks (DNNs) have made tremendous progress in many different areas in recent years. How these networks function internally, however, is often not well understood. Advances in under-standing DNNs will benefit and accelerate the development of the field. We present TNNVis, a visualization system that supports un-derstanding of deep neural networks specifically designed to analyze text. TNNVis focuses on DNNs composed of fully connected and convolutional layers. It integrates visual encodings and interaction techniques chosen specifically for our tasks. The tool allows users to: (1) visually explore DNN models with arbitrary input using a combination of node-link diagrams and matrix representation; (2) quickly identify activation values, weights, and feature map patterns within a network; (3) flexibly focus on visual information of interest with threshold, inspection, insight query, and tooltip operations; (4) discover network activation and training patterns through animation; and (5) compare differences between internal activation patterns for different inputs to the DNN. These functions allow neural network researchers to examine their DNN models from new perspectives, producing insights on how these models function. Clustering and summarization techniques are employed to support large convolutional and fully connected layers. Based on several part of speech models with different structure and size, we present multiple use cases where visualization facilitates an understanding of the models.",
        "author": "Nie, Shaoliang and Healey, Christopher and Padia, Kalpesh and Leeman-Munk, Samuel and Benson, Jordan and Caira, Dave and Sethi, Saratendu and Devarajan, Ravi",
        "booktitle": "2018 IEEE Pacific Visualization Symposium",
        "doi": "10.1109/PacificVis.2018.00031",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare",
        "organization": "IEEE",
        "pages": "180--189",
        "series": "PacificVis",
        "title": "Visualizing Deep Neural Networks for Text Analytics",
        "type": "inproceedings",
        "url": "https://www.csc2.ncsu.edu/faculty/healey/download/pacvis.18.pdf",
        "year": "2018"
    },
    "nourani2022detoxer": {
        "abstract": "In many applications, developed deep-learning models need to be iteratively debugged and refined to improve the model efficiency over time. Debugging some models, such as temporal multilabel classification (TMLC) where each data point can simultaneously belong to multiple classes, can be especially more challenging due to the complexity of the analysis and instances that need to be reviewed. In this article, focusing on video activity recognition as an application of TMLC, we propose DETOXER , an interactive visual debugging system to support finding different error types and scopes through providing multiscope explanations.",
        "author": "Nourani, Mahsan and Roy, Chiradeep and Honeycutt, Donald R and Ragan, Eric D and Gogate, Vibhav",
        "doi": "10.1109/MCG.2022.3201465",
        "journal": "IEEE Computer Graphics and Applications",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "6",
        "pages": "37--46",
        "publisher": "IEEE",
        "series": "CG\\&A",
        "title": "Detoxer: a visual debugging tool with multi-scope explanations for temporal multi-label classification",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9866547",
        "volume": "42",
        "year": "2022"
    },
    "park2019comdia": {
        "abstract": "Performance analysis is essential for improving classification models. However, existing performance analysis tools do not provide actionable insights such as the cause of misclassification. Machine learning practitioners face difficulties such as prioritizing model, looking over confusion between classes. In addition, existing performance analysis tools that provide feature-level analysis are difficult to apply to image classification problems. This study has been proposed to solve these difficulties. In this paper, we present an interactive visual analytics system for diagnosing the performance of multiclass classification models. Our system is able to compare multiple models, find weaknesses, and obtain actionable insights for improving models. Our visualization consists of three views for analyzing performance at the class, confusion, and instance levels. We demonstrate our system using MNIST handwritten digits data.",
        "author": "Park, Chanhee and Lee, Jina and Han, Hyunwoo and Lee, Kyungwon",
        "booktitle": "2019 IEEE Pacific Visualization Symposium (PacificVis)",
        "doi": "10.1109/PacificVis.2019.00044",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "organization": "IEEE",
        "pages": "313--317",
        "series": "PacificVis Notes",
        "title": "ComDia+: An Interactive Visual Analytics System for Comparing, Diagnosing, and Improving Multiclass Classifiers",
        "type": "inproceedings",
        "url": "https://chanhee-park.github.io/assets/researches/2019-pvis-comdia.pdf",
        "year": "2019"
    },
    "park2019sanvis": {
        "abstract": "Attention networks, a deep neural network architecture inspired by humans' attention mechanism, have seen significant success in image captioning, machine translation, and many other applications. Recently, they have been further evolved into an advanced approach called multi-head self-attention networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether multi-head attention. Meanwhile, the increased model complexity prevents users from easily understanding and manipulating the inner workings of models. To tackle the challenges, we present a visual analytics system called SANVis, which helps users understand the behaviors and the characteristics of multi-head self-attention networks. Using a state-of-the-art self-attention model called Transformer, we demonstrate usage scenarios of SANVis in machine translation tasks. Our system is available at http://short.sanvis.org.",
        "author": "Park, Cheonbok and Na, Inyoup and Jo, Yongjang and Shin, Sungbok and Yoo, Jaehyo and Kwon, Bum Chul and Zhao, Jian and Noh, Hyungjong and Lee, Yeonsoo and Choo, Jaegul",
        "booktitle": "2019 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VISUAL.2019.8933677",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore",
        "organization": "IEEE",
        "pages": "146--150",
        "series": "VIS Short",
        "title": "SANVis: Visual Analytics for Understanding Self-Attention Networks",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/1909.09595.pdf",
        "year": "2019"
    },
    "park2021neurocartography": {
        "abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \u201cdog faces\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \u201cdog face\u201d and \u201cdog tail\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.",
        "author": "Park, Haekyu and Das, Nilaksh and Duggal, Rahul and Wright, Austin P and Shaikh, Omar and Hohman, Fred and Chau, Duen Horng Polo",
        "doi": "10.1109/TVCG.2021.3114858",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore",
        "number": "1",
        "pages": "813--823",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Neurocartography: Scalable automatic visual summarization of concepts in deep neural networks",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552879",
        "volume": "28",
        "year": "2021"
    },
    "park2022vant": {
        "abstract": "The quality of parallel corpora used to train a Neural Machine Translation (NMT) model can critically influence the model's performance. Various approaches for refining parallel corpora have been introduced, but there is still much room for improvements, such as enhancing the efficiency and the quality of refinement. We introduce VANT, a novel visual analytics system for refining parallel corpora used in training an NMT model. Our system helps users to readily detect and filter noisy parallel corpora by (1) aiding the quality estimation of individual sentence pairs within the corpora by providing diverse quality metrics (e.g., cosine similarity, BLEU, length ratio) and (2) allowing users to visually examine and manage the corpora based on the pre-computed metrics scores. Our system's effectiveness and usefulness are demonstrated through a qualitative user study with eight participants, including four domain experts with real-world datasets.",
        "author": "Park, Sebeom and Lee, Soohyun and Kim, Youngtaek and Jeon, Hyeon and Jung, Seokweon and Bok, Jinwook and Seo, Jinwook",
        "booktitle": "2022 IEEE 15th Pacific Visualization Symposium (PacificVis)",
        "doi": "10.1109/PacificVis53943.2022.00029",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "organization": "IEEE",
        "pages": "181--185",
        "series": "PacificVis Notes",
        "title": "VANT: A Visual Analytics System for Refining Parallel Corpora in Neural Machine Translation",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9787874",
        "year": "2022"
    },
    "pezzotti2017deepeyes": {
        "abstract": "Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.",
        "author": "Pezzotti, Nicola and H{\\\"o}llt, Thomas and Van Gemert, Jan and Lelieveldt, Boudewijn PF and Eisemann, Elmar and Vilanova, Anna",
        "doi": "10.1109/TVCG.2017.2744358",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore",
        "number": "1",
        "pages": "98--108",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks",
        "type": "article",
        "url": "https://graphics.tudelft.nl/Publications-new/2018/PHVLEV18/paper216.pdf",
        "volume": "24",
        "year": "2017"
    },
    "puhringer2020instanceflow": {
        "abstract": "Classification is one of the most important supervised machine learning tasks. During the training of a classification model, the training instances are fed to the model multiple times (during multiple epochs) in order to iteratively improve classification performance. The increasing complexity of models has led to a growing demand to make them interpretable through visualization. Existing approaches mostly focus on the visual analysis of the final model performance after training and are often limited to aggregate performance measures. In this paper, we introduce InstanceFlow, a novel dual-view visualization tool that allows users to analyze the learning behavior of classifiers over time at the instance-level. A Sankey diagram visualizes the flow of instances throughout epochs, with on-demand detailed glyphs and traces for individual instances. A tabular view allows users to locate interesting instances by ranking and filtering. Thus, InstanceFlow bridges the gap between class-level and instance-level performance evaluation while enabling users to perform a full temporal analysis of the training process.",
        "author": "P{\\\"u}hringer, Michael and Hinterreiter, Andreas and Streit, Marc",
        "booktitle": "2020 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VIS47514.2020.00065",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Assess",
        "pages": "291--295",
        "series": "Vis Short",
        "title": "InstanceFlow: Visualizing the Evolution of Classifier Confusion at the Instance Level",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/2007.11353.pdf",
        "year": "2020"
    },
    "puri2020rankbooster": {
        "abstract": "Ranking is a natural and ubiquitous way to facilitate decision-making in various applications. However, different rankings are often used for the same set of entities, with each ranking method placing emphasis on different factors. These factors can also be multi-dimensional in nature, compounding the problem. This complexity can make it challenging for an entity which is being ranked to understand what they can do to improve their rankings, and to analyze the effect of changes in various factors to their overall rank. In this paper, we present RankBooster, a novel visual analytics system to help users conveniently investigate ranking predictions.We take university rankings as an example and focus on helping universities to better explore their rankings, where they can compare themselves to their rivals in key areas as well as overall. Novel visualizations are proposed to enable efficient analysis of rankings, including a Scenario Analysis View to show a high-level summary of different ranking scenarios, a Relationship View to visualize the influence of each attribute on different indicators and a Rival View to compare the ranking of a university and those of its rivals. A case study demonstrates the usefulness and effectiveness of RankBooster in facilitating the visual analysis of ranking predictions and helping users better understand their current situation.",
        "author": "Puri, Abishek and Ku, Bon Kyung and Wang, Yong and Qu, Huamin",
        "doi": "10.2312/evs.20201068",
        "journal": "Proceedings of EuroVis 2020 Short Papers",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "pages": "175--179",
        "publisher": "The Eurographics Association",
        "series": "EuroVis (short)",
        "title": "Rankbooster: Visual analysis of ranking predictions",
        "type": "article",
        "url": "https://diglib.eg.org/handle/10.2312/evs20201068",
        "year": "2020"
    },
    "rathore2021topoact": {
        "abstract": "Deep neural networks such as GoogLeNet, ResNet, and BERT have achieved impressive performance in tasks such as image and text classification. To understand how such performance is achieved, we probe a trained deep neural network by studying neuron activations, i.e.combinations of neuron firings, at various layers of the network in response to a particular input. With a large number of inputs, we aim to obtain a global view of what neurons detect by studying their activations. In particular, we develop visualizations that show the shape of the activation space, the organizational principle behind neuron activations, and the relationships of these activations within a layer. Applying tools from topological data analysis, we present TopoAct, a visual exploration system to study topological summaries of activation vectors. We present exploration scenarios using TopoAct that provide valuable insights into learned representations of neural networks. We expect TopoAct to give a topological perspective that enriches the current toolbox of neural network analysis, and to provide a basis for network architecture diagnosis and data anomaly detection.",
        "author": "Rathore, Archit and Chalapathi, Nithin and Palande, Sourabh and Wang, Bei",
        "booktitle": "Computer Graphics Forum",
        "doi": "doi.org/10.1111/cgf.14195",
        "keywords": "type:DL,data:Sequence,data:MD-Array,task:Present,task:Explore,task:Compare",
        "number": "1",
        "organization": "Wiley Online Library",
        "pages": "382--397",
        "series": "CGF",
        "title": "TopoAct: Visually exploring the shape of activations in deep learning",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14195",
        "volume": "40",
        "year": "2021"
    },
    "rauber2016visualizing": {
        "abstract": "In machine learning, pattern classification assigns high-dimensional vectors (observations) to classes based on generalization from examples. Artificial neural networks currently achieve state-of-the-art results in this task. Although such networks are typically used as black-boxes, they are also widely believed to learn (high-dimensional) higher-level representations of the original observations. In this paper, we propose using dimensionality reduction for two tasks: visualizing the relationships between learned representations of observations, and visualizing the relationships between artificial neurons. Through experiments conducted in three traditional image classification benchmark datasets, we show how visualization can provide highly valuable feedback for network designers. For instance, our discoveries in one of these datasets (SVHN) include the presence of interpretable clusters of learned representations, and the partitioning of artificial neurons into groups with apparently related discriminative roles.",
        "author": "Rauber, Paulo E and Fadel, Samuel G and Falcao, Alexandre X and Telea, Alexandru C",
        "doi": "10.1109/TVCG.2016.2598838",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare",
        "number": "1",
        "pages": "101--110",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Visualizing the Hidden Activity of Artificial Neural Networks",
        "type": "article",
        "url": "https://www.researchgate.net/profile/Samuel\\_Fadel/publication/306049229\\_Visualizing\\_the\\_Hidden\\_Activity\\_of\\_Artificial\\_Neural\\_Networks/links/5b13ffa7aca2723d9980083c/Visualizing-the-Hidden-Activity-of-Artificial-Neural-Networks.pdf",
        "volume": "23",
        "year": "2016"
    },
    "ren2016squares": {
        "abstract": "Performance analysis is critical in applied machine learning because it influences the models practitioners produce. Current performance analysis tools suffer from issues including obscuring important characteristics of model behavior and dissociating performance from data. In this work, we present Squares, a performance visualization for multiclass classification problems. Squares supports estimating common performance metrics while displaying instance-level distribution information necessary for helping practitioners prioritize efforts and access data. Our controlled study shows that practitioners can assess performance significantly faster and more accurately with Squares than a confusion matrix, a common performance analysis tool in machine learning.",
        "author": "Ren, Donghao and Amershi, Saleema and Lee, Bongshin and Suh, Jina and Williams, Jason D",
        "doi": "10.1109/TVCG.2016.2598828",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "61--70",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Squares: Supporting Interactive Performance Analysis for Multiclass Classifiers",
        "type": "article",
        "url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/09/squares.VAST\\_.2016-1.pdf",
        "volume": "23",
        "year": "2016"
    },
    "roesch2019visualization": {
        "abstract": "Recurrent neural networks are prime candidates for learning evolutions in multi\u2010dimensional time series data. The performance of such a network is judged by the loss function, which is aggregated into a scalar value that decreases during training. Observing only this number hides the variation that occurs within the typically large training and testing data sets. Understanding these variations is of highest importance to adjust network hyper\u2010parameters, such as the number of neurons, number of layers or to adjust the training set to include more representative examples. In this paper, we design a comprehensive and interactive system that allows users to study the output of recurrent neural networks on both the complete training data and testing data. We follow a coarse\u2010to\u2010fine strategy, providing overviews of annual, monthly and daily patterns in the time series and directly support a comparison of different hyper\u2010parameter settings. We applied our method to a recurrent convolutional neural network that was trained and tested on 25 years of climate data to forecast meteorological attributes, such as temperature, pressure and wind velocity. We further visualize the quality of the forecasting models, when applied to various locations on the Earth and we examine the combination of several forecasting models.",
        "author": "Roesch, Isabelle and G{\\\"u}nther, Tobias",
        "booktitle": "Computer Graphics Forum",
        "doi": "https://doi.org/10.1111/cgf.13453",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "organization": "Wiley Online Library",
        "pages": "209--220",
        "series": "CGF",
        "title": "Visualization of Neural Network Predictions for Weather Forecasting",
        "type": "inproceedings",
        "url": "https://www.researchgate.net/profile/Tobias\\_Guenther5/publication/319092627\\_Visualization\\_of\\_Neural\\_Network\\_Predictions\\_for\\_Weather\\_Forecasting/links/5b361a1d4585150d23e1079c/Visualization-of-Neural-Network-Predictions-for-Weather-Forecasting.pdf",
        "volume": "38",
        "year": "2019"
    },
    "rojo2020gacovi": {
        "abstract": "The recent growth of interest in explainable artificial intelligence (XAI) has resulted in a large number of research efforts to provide accountable and transparent machine learning systems. Although a large volume of research has focused on algorithm transparency, there are other factors that influence the interpretability of a system, such as end-users' understanding of individual features and the total number of features. Thus, involving end-users in the feature selection process may be key to achieving interpretability. In addition, previous work has suggested that to obtain satisfactory interpretability and predictive performance, the feature selection process should look for a subset of features that are highly correlated with the response variable yet uncorrelated to each other. Taking this into account, in this paper, we present a work-in-progress design study of a novel system for correlation visualization, GaCoVi. GaCoVi is designed to put domain experts in the loop of feature selection for regression models in scenarios where transparency of the machine learning systems is crucial.",
        "author": "Rojo, Diego and Htun, Nyi Nyi and Verbert, Katrien and Kerren, Andreas and Garth, Christoph and Marai, G Elisabeta",
        "doi": "10.2312/evs.20201060",
        "journal": "Proceedings of EuroVis 2020 Short Papers",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Assess",
        "pages": "127--131",
        "publisher": "The Eurographics Association",
        "series": "EuroVis (short)",
        "title": "GaCoVi: A correlation visualization to support interpretability-aware feature selection for regression models",
        "type": "article",
        "url": "https://diglib.eg.org/handle/10.2312/evs20201060",
        "year": "2020"
    },
    "sahoo2020visually": {
        "abstract": "We propose a visual analytics system to help a user analyze and steer zero-shot learning models. Zero-shot learning has emerged as a viable scenario for categorizing data that consists of no labeled examples, and thus a promising approach to minimize data annotation from humans. However, it is challenging to understand where zero-shot learning fails, the cause of such failures, and how a user can modify the model to prevent such failures. Our visualization system is designed to help users diagnose and understand mispredictions in such models, so that they may gain insight on the behavior of a model when applied to data associated with categories not seen during training. Through usage scenarios, we highlight how our system can help a user improve performance in zero-shot learning.",
        "author": "Sahoo, Saroj and Berger, Matthew",
        "booktitle": "2020 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VIS47514.2020.00057",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore",
        "pages": "251--255",
        "series": "Vis Short",
        "title": "Visually Analyzing and Steering Zero Shot Learning",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/2009.05254.pdf",
        "year": "2020"
    },
    "sawatzky2019visualizing": {
        "abstract": "Recurrent Neural Networks are an effective and prevalent tool used to model sequential data such as natural language text. However, their deep nature and massive number of parameters pose a challenge for those intending to study precisely how they work. We present a visual technique that gives a high level intuition behind the semantics of the hidden states within Recurrent Neural Networks. This semantic encoding allows for hidden states to be compared throughout the model independent of their internal details. The proposed technique is displayed in a proof of concept visualization tool which is demonstrated to visualize the natural language processing task of language modelling.",
        "author": "Sawatzky, Lindsey and Bergner, Steven and Popowich, Fred",
        "booktitle": "2019 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VISUAL.2019.8933744",
        "keywords": "type:DL,data:Sequence,task:Present",
        "organization": "IEEE",
        "pages": "156--160",
        "series": "VIS Short",
        "title": "Visualizing RNN States with Predictive Semantic Encodings",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/1908.00588.pdf",
        "year": "2019"
    },
    "sevastjanova2022lmfingerprints": {
        "abstract": "Language models, such as BERT, construct multiple, contextualized embeddings for each word occurrence in a corpus. Understanding how the contextualization propagates through the model's layers is crucial for deciding which layers to use for a specific analysis task. Currently, most embedding spaces are explained by probing classifiers; however, some findings remain inconclusive. In this paper, we present LMFingerprints, a novel scoring-based technique for the explanation of contextualized word embeddings. We introduce two categories of scoring functions, which measure (1) the degree of contextualization, i.e., the layerwise changes in the embedding vectors, and (2) the type of contextualization, i.e., the captured context information. We integrate these scores into an interactive explanation workspace. By combining visual and verbal elements, we provide an overview of contextualization in six popular transformer-based language models. We evaluate hypotheses from the domain of computational linguistics, and our results not only confirm findings from related work but also reveal new aspects about the information captured in the embedding spaces. For instance, we show that while numbers are poorly contextualized, stopwords have an unexpected high contextualization in the models' upper layers, where their neighborhoods shift from similar functionality tokens to tokens that contribute to the meaning of the surrounding sentences.",
        "author": "Sevastjanova, Rita and Kalouli, A and Beck, Christin and Hauptmann, Hanna and El-Assady, Mennatallah",
        "booktitle": "Computer Graphics Forum",
        "doi": "doi.org/10.1111/cgf.14541",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "3",
        "organization": "Wiley Online Library",
        "pages": "295--307",
        "series": "EuroVis",
        "title": "LMFingerprints: Visual Explanations of Language Model Embedding Spaces through Layerwise Contextualization Scores",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14541",
        "volume": "41",
        "year": "2022"
    },
    "shao2021visual": {
        "abstract": "Lithium ion batteries (LIBs) are widely used as the important energy sources in our daily life such as mobile phones, electric vehicles, and drones etc. Due to the potential safety risks caused by liquid electrolytes, the experts have tried to replace liquid electrolytes with solid ones. However, it is very difficult to find suitable alternatives materials in traditional ways for its incredible high cost in searching. Machine learning (ML) based methods are currently introduced and used for material prediction. But there is rarely an assisting learning tools designed for domain experts for institutive performance comparison and analysis of ML model. In this case, we propose an interactive visualization system for experts to select suitable ML models, understand and explore the predication results comprehensively. Our system employs a multi-faceted visualization scheme designed to support analysis from the perspective of feature composition, data similarity, model performance, and results presentation. A case study with real experiments in lab has been taken by the expert and the results of confirmed the effectiveness and helpfulness of our system.",
        "author": "Shao, Hui and Pu, Jiansu and Zhu, Yanlin and Gao, Boyang and Zhu, Zhengguo and Rao, Yunbo",
        "booktitle": "2021 IEEE 14th Pacific Visualization Symposium (PacificVis)",
        "doi": "10.1109/PacificVis52677.2021.00038",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "organization": "IEEE",
        "pages": "1--5",
        "series": "PacificVis Notes",
        "title": "Visual Analysis on Machine Learning Assisted Prediction of Ionic Conductivity for Solid-State Electrolytes",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9438763",
        "year": "2021"
    },
    "shen2020visual": {
        "abstract": "Recent attempts at utilizing visual analytics to interpret Recurrent Neural Networks (RNNs) mainly focus on natural language processing (NLP) tasks that take symbolic sequences as input. However, many real-world problems like environment pollution forecasting apply RNNs on sequences of multi-dimensional data where each dimension represents an individual feature with semantic meaning such as PM 2.5 and SO 2 . RNN interpretation on multi-dimensional sequences is challenging as users need to analyze what features are important at different time steps to better understand model behavior and gain trust in prediction. This requires effective and scalable visualization methods to reveal the complex many-to-many relations between hidden units and features. In this work, we propose a visual analytics system to interpret RNNs on multi-dimensional time-series forecasts. Specifically, to provide an overview to reveal the model mechanism, we propose a technique to estimate the hidden unit response by measuring how different feature selections affect the hidden unit output distribution. We then cluster the hidden units and features based on the response embedding vectors. Finally, we propose a visual analytics system which allows users to visually explore the model behavior from the global and individual levels. We demonstrate the effectiveness of our approach with case studies using air pollutant forecast applications.",
        "author": "Shen, Qiaomu and Wu, Yanhong and Jiang, Yuzhe and Zeng, Wei and Alexis, KH and Vianova, Anna and Qu, Huamin",
        "booktitle": "2020 IEEE Pacific Visualization Symposium",
        "doi": "10.1109/PacificVis48177.2020.2785",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Compare",
        "organization": "IEEE",
        "pages": "61--70",
        "series": "PacificVis",
        "title": "Visual Interpretation of Recurrent Neural Network on Multi-dimensional Time-series Forecast",
        "type": "inproceedings",
        "url": "https://graphics.tudelft.nl/Publications-new/2020/SWJZLVQ20/pacificvis20a-sub2785-cam-i5.pdf",
        "year": "2020"
    },
    "sietzen2021interactive": {
        "abstract": "While convolutional neural networks (CNNs) have found wide adoption as state-of-the-art models for image-related tasks, their predictions are often highly sensitive to small input perturbations, which the human vision is robust against. This paper presents Perturber, a web-based application that allows users to instantaneously explore how CNN activations and predictions evolve when a 3D input scene is interactively perturbed. Perturber offers a large variety of scene modifications, such as camera controls, lighting and shading effects, background modifications, object morphing, as well as adversarial attacks, to facilitate the discovery of potential vulnerabilities. Fine-tuned model versions can be directly compared for qualitative evaluation of their robustness. Case studies with machine learning experts have shown that Perturber helps users to quickly generate hypotheses about model vulnerabilities and to qualitatively compare model behavior. Using quantitative analyses, we could replicate users\u2019 insights with other CNN architectures and input images, yielding new insights about the vulnerability of adversarially trained models.",
        "author": "Sietzen, Stefan and Lechner, Mathias and Borowski, Judy and Hasani, Ramin and Waldner, Manuela",
        "booktitle": "Computer Graphics Forum",
        "doi": "doi.org/10.1111/cgf.14418",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Generate",
        "number": "7",
        "organization": "Wiley Online Library",
        "pages": "253--264",
        "series": "CGF",
        "title": "Interactive analysis of CNN robustness",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14418",
        "volume": "40",
        "year": "2021"
    },
    "stahnke2015probing": {
        "abstract": "We introduce a set of integrated interaction techniques to interpret and interrogate dimensionality-reduced data. Projection techniques generally aim to make a high-dimensional information space visible in form of a planar layout. However, the meaning of the resulting data projections can be hard to grasp. It is seldom clear why elements are placed far apart or close together and the inevitable approximation errors of any projection technique are not exposed to the viewer. Previous research on dimensionality reduction focuses on the efficient generation of data projections, interactive customisation of the model, and comparison of different projection techniques. There has been only little research on how the visualization resulting from data projection is interacted with. We contribute the concept of probing as an integrated approach to interpreting the meaning and quality of visualizations and propose a set of interactive methods to examine dimensionality-reduced data as well as the projection itself. The methods let viewers see approximation errors, question the positioning of elements, compare them to each other, and visualize the influence of data dimensions on the projection space. We created a web-based system implementing these methods, and report on findings from an evaluation with data analysts using the prototype to examine multidimensional datasets.",
        "author": "Stahnke, Julian and D{\\{o}rk, Marian and M{\\}u}ller, Boris and Thom, Andreas",
        "doi": "10.1109/TVCG.2015.2467717",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "629--638",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Probing projections: Interaction techniques for interpreting arrangements and errors of dimensionality reductions",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/7192695",
        "volume": "22",
        "year": "2016"
    },
    "strobelt2017lstmvis": {
        "abstract": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks. Long-term usage data after putting the tool online revealed great interest in the machine learning community.",
        "author": "Strobelt, Hendrik and Gehrmann, Sebastian and Pfister, Hanspeter and Rush, Alexander M",
        "doi": "10.1109/TVCG.2017.2744158",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore",
        "number": "1",
        "pages": "667--676",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks",
        "type": "article",
        "url": "https://arxiv.org/pdf/1606.07461.pdf",
        "volume": "24",
        "year": "2017"
    },
    "strobelt2018s": {
        "abstract": "Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and \u201cwhat if\u201d-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.",
        "author": "Strobelt, Hendrik and Gehrmann, Sebastian and Behrisch, Michael and Perer, Adam and Pfister, Hanspeter and Rush, Alexander M",
        "doi": "10.1109/TVCG.2018.2865044",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Compare",
        "number": "1",
        "pages": "353--363",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models",
        "type": "article",
        "url": "https://arxiv.org/pdf/1804.09299.pdf",
        "volume": "25",
        "year": "2018"
    },
    "strobelt2021genni": {
        "abstract": "Table2Text systems generate textual output based on structured data utilizing machine learning. These systems are essential for fluent natural language interfaces in tools such as virtual assistants; however, left to generate freely these ML systems often produce misleading or unexpected outputs. GenNI (Generation Negotiation Interface) is an interactive visual system for high-level human-AI collaboration in producing descriptive text. The tool utilizes a deep learning model designed with explicit control states. These controls allow users to globally constrain model generations, without sacrificing the representation power of the deep learning models. The visual interface makes it possible for users to interact with AI systems following a Refine-Forecast paradigm to ensure that the generation system acts in a manner human users find suitable. We report multiple use cases on two experiments that improve over uncontrolled generation approaches, while at the same time providing fine-grained control. A demo and source code are available at https://genni.vizhub.ai.",
        "author": "Strobelt, Hendrik and Kinley, Jambay and Krueger, Robert and Beyer, Johanna and Pfister, Hanspeter and Rush, Alexander M",
        "doi": "10.1109/TVCG.2021.3114845",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Assess,task:Improve",
        "number": "1",
        "pages": "1106--1116",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "GenNI: Human-AI Collaboration for Data-Backed Text Generation",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552430",
        "volume": "28",
        "year": "2021"
    },
    "strobelt2022interactive": {
        "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai ) and our workflow using several real-world use cases.",
        "author": "Strobelt, Hendrik and Webson, Albert and Sanh, Victor and Hoover, Benjamin and Beyer, Johanna and Pfister, Hanspeter and Rush, Alexander M",
        "doi": "10.1109/TVCG.2022.3209479",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:DL,data:Sequence,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "1146--1156",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9908590",
        "volume": "29",
        "year": "2022"
    },
    "tabatabai2021did": {
        "abstract": "How can we help domain-knowledgeable users who do not have expertise in AI analyze why an AI agent failed? Our research team previously developed a new structured process for such users to assess AI, called After-Action Review for AI (AAR/AI), consisting of a series of steps a human takes to assess an AI agent and formalize their understanding. In this paper, we investigate how the AAR/AI process can scale up to support reinforcement learning (RL) agents that operate in complex environments. We augment the AAR/AI process to be performed at three levels\u2014episode-level, decision-level, and explanation-level\u2014and integrate it into our redesigned visual analytics interface. We illustrate our approach through a usage scenario of analyzing why a RL agent lost in a complex real-time strategy game built with the StarCraft 2 engine. We believe integrating structured processes like AAR/AI into visualization tools can help visualization play a more critical role in AI interpretability.",
        "author": "Tabatabai, Delyar and Ruangrotsakun, Anita and Irvine, Jed and Dodge, Jonathan and Shureih, Zeyad and Lam, Kin-Ho and Burnett, Margaret and Fern, Alan and Kahng, Minsuk",
        "booktitle": "2021 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VIS49827.2021.9623268",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Assess",
        "organization": "IEEE",
        "pages": "16--20",
        "series": "VIS Short",
        "title": "\u201cWhy did my AI agent lose?\u201d: Visual Analytics for Scaling Up After-Action Review",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9623268",
        "year": "2021"
    },
    "wang2018dqnviz": {
        "abstract": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.",
        "author": "Wang, Junpeng and Gou, Liang and Shen, Han-Wei and Yang, Hao",
        "doi": "10.1109/TVCG.2018.2864504",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "288--298",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "{DQNViz}: A Visual Analytics Approach to Understand Deep Q-Networks",
        "type": "article",
        "url": "https://junpengw.bitbucket.io/image/research/vast18.pdf",
        "volume": "25",
        "year": "2018"
    },
    "wang2018ganviz": {
        "abstract": "Generative models bear promising implications to learn data representations in an unsupervised fashion with deep learning. Generative Adversarial Nets (GAN) is one of the most popular frameworks in this arena. Despite the promising results from different types of GANs, in-depth understanding on the adversarial training process of the models remains a challenge to domain experts. The complexity and the potential long-time training process of the models make it hard to evaluate, interpret, and optimize them. In this work, guided by practical needs from domain experts, we design and develop a visual analytics system, GANViz, aiming to help experts understand the adversarial process of GANs in-depth. Specifically, GANViz evaluates the model performance of two subnetworks of GANs, provides evidence and interpretations of the models' performance, and empowers comparative analysis with the evidence. Through our case studies with two real-world datasets, we demonstrate that GANViz can provide useful insight into helping domain experts understand, interpret, evaluate, and potentially improve GAN models.",
        "author": "Wang, Junpeng and Gou, Liang and Yang, Hao and Shen, Han-Wei",
        "doi": "10.1109/TVCG.2018.2816223",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "6",
        "pages": "1905--1917",
        "publisher": "IEEE",
        "series": "PacificVis",
        "title": "{GANViz}: A Visual Analytics Approach to Understand the Adversarial Game",
        "type": "article",
        "url": "https://junpengw.bitbucket.io/image/research/pvis18.pdf",
        "volume": "24",
        "year": "2018"
    },
    "wang2019deepvid": {
        "abstract": "Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID.",
        "author": "Wang, Junpeng and Gou, Liang and Zhang, Wei and Yang, Hao and Shen, Han-Wei",
        "doi": "10.1109/TVCG.2019.2903943",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Assess,task:Generate",
        "number": "6",
        "pages": "2168--2180",
        "publisher": "IEEE",
        "series": "PacificVis",
        "title": "{DeepVID}: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation",
        "type": "article",
        "url": "https://junpengw.bitbucket.io/image/research/pvis19.pdf",
        "volume": "25",
        "year": "2019"
    },
    "wang2020cnn": {
        "abstract": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.",
        "author": "Wang, Zijie J and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng Polo",
        "doi": "10.1109/TVCG.2020.3030418",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore",
        "number": "2",
        "pages": "1396--1406",
        "series": "VIS",
        "title": "CNN explainer: Learning convolutional neural networks with interactive visualization",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "27",
        "year": "2020"
    },
    "wang2020conceptexplorer": {
        "abstract": "Time-series data is widely studied in various scenarios, like weather forecast, stock market, customer behavior analysis. To comprehensively learn about the dynamic environments, it is necessary to comprehend features from multiple data sources. This paper proposes a novel visual analysis approach for detecting and analyzing concept drifts from multi-sourced time-series. We propose a visual detection scheme for discovering concept drifts from multiple sourced time-series based on prediction models. We design a drift level index to depict the dynamics, and a consistency judgment model to justify whether the concept drifts from various sources are consistent. Our integrated visual interface, ConceptExplorer, facilitates visual exploration, extraction, understanding, and comparison of concepts and concept drifts from multi-source time-series data. We conduct three case studies and expert interviews to verify the effectiveness of our approach.",
        "author": "Wang, Xumeng and Chen, Wei and Xia, Jiazhi and Chen, Zexian and Xu, Dongshi and Wu, Xiangyang and Xu, Mingliang and Schreck, Tobias",
        "booktitle": "2020 IEEE Conference on Visual Analytics Science and Technology (VAST)",
        "doi": "10.1109/VAST50239.2020.00006",
        "keywords": "type:General ML,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "organization": "IEEE",
        "pages": "1--11",
        "series": "VIS (VAST)",
        "title": "ConceptExplorer: Visual analysis of concept drifts in multi-source time-series data",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9308627",
        "year": "2020"
    },
    "wang2020hypoml": {
        "abstract": "In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a \u201cconcept\u201d or \u201cfeature\u201d may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.",
        "author": "Wang, Qianwen and Alexander, William and Pegg, Jack and Qu, Huamin and Chen, Min",
        "doi": "10.1109/TVCG.2020.3030449",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "number": "2",
        "pages": "1417--1426",
        "series": "VIS",
        "title": "HypoML: Visual analysis for hypothesis-based evaluation of machine learning models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "27",
        "year": "2020"
    },
    "wang2020scanviz": {
        "abstract": "Two fundamental problems in machine learning are recognition and generation. Apart from the tremendous amount of research efforts devoted to these two problems individually, finding the association between them has attracted increasingly more attention recently. Symbol-Concept Association Network (SCAN) is one of the most popular models for this problem proposed by Google DeepMind lately, which integrates an unsupervised concept abstraction process and a supervised symbol-concept association process. Despite the outstanding performance of this deep neural network, interpreting and evaluating it remain challenging. Guided by the practical needs from deep learning experts, this paper proposes a visual analytics attempt, i.e., SCANViz, to address this challenge in the visual domain. Specifically, SCANViz evaluates the performance of SCAN through its power of recognition and generation, facilitates the exploration of the latent space derived from both the unsupervised extraction and supervised association processes, empowers interactive training of SCAN to interpret the model's understanding on a particular visual concept. Through concrete case studies with multiple deep learning experts, we validate the effectiveness of SCANViz.",
        "author": "Wang, Junpeng and Zhang, Wei and Yang, Hao",
        "booktitle": "2020 IEEE Pacific Visualization Symposium",
        "doi": "10.1109/PacificVis48177.2020.3542",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Assess",
        "organization": "IEEE",
        "pages": "51--60",
        "series": "PacificVis",
        "title": "{SCANViz}: Interpreting the Symbol-Concept Association Captured by Deep Neural Networks through Visual Analytics",
        "type": "inproceedings",
        "url": "https://junpengw.bitbucket.io/image/research/pvis20.pdf",
        "year": "2020"
    },
    "wang2020visual": {
        "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.",
        "author": "Wang, Qianwen and Xu, Zhenhua and Chen, Zhutian and Wang, Yong and Liu, Shixia and Qu, Huamin",
        "doi": "10.1109/TVCG.2020.3030471",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "2",
        "pages": "1470--1480",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Visual analysis of discrimination in machine learning",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9222272",
        "volume": "27",
        "year": "2020"
    },
    "wang2021investigating": {
        "abstract": "Tree boosting models are widely adopted predictive models and have demonstrated superior performance than other conventional and even deep learning models, especially since the recent release of their parallel and distributed implementations, e.g., XGBoost, LightGMB, and CatBoost. Tree boosting uses a group of sequentially generated weak learners (i.e., decision trees), each learns from the mistakes of its predecessor, to push the model\u2019s decision boundary towards the true boundary. As the number of trees keeps increasing over training, it is important to reveal how the newly-added trees change the predictions of individual data instances, and how the impacts of different data features evolve. To accomplish these goals, in this paper, we introduce a new design of the temporal confusion matrix, providing users with an effective interface to track data instances\u2019 predictions across the tree boosting process. Also, we present an improved visualization to better illustrate and compare the impacts of individual data features (based on their SHAP values) across training iterations. Integrating these components with a tree structure visualization component, we propose a visual analytics system for tree boosting models. Through case studies with domain experts using real-world datasets, we validated the system\u2019s effectiveness.",
        "author": "Wang, Junpeng and Zhang, Wei and Wang, Liang and Yang, Hao",
        "booktitle": "2021 IEEE 14th Pacific Visualization Symposium (PacificVis)",
        "doi": "10.1109/PacificVis52677.2021.00032",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "organization": "IEEE",
        "pages": "186--195",
        "series": "PacificVis",
        "title": "Investigating the evolution of tree boosting models with visual analytics",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9438786",
        "year": "2021"
    },
    "wang2021m2lens": {
        "abstract": "Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2 Lens, to visualize and explain multimodal models for sentiment analysis. M2 Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2 Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",
        "author": "Wang, Xingbo and He, Jianben and Jin, Zhihua and Yang, Muqiao and Wang, Yong and Qu, Huamin",
        "doi": "10.1109/TVCG.2021.3114794",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "802--812",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "M2Lens: visualizing and explaining multimodal models for sentiment analysis",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552921",
        "volume": "28",
        "year": "2021"
    },
    "wang2021visual": {
        "abstract": "Deep reinforcement learning (DRL) targets to train an autonomous agent to interact with a pre-defined environment and strives to achieve specific goals through deep neural networks (DNN). Recurrent neural network (RNN) based DRL has demonstrated superior performance, as RNNs can effectively capture the temporal evolution of the environment and respond with proper agent actions. However, apart from the outstanding performance, little is known about how RNNs understand the environment internally and what has been memorized over time. Revealing these details is extremely important for deep learning experts to understand and improve DRLs, which in contrast, is also challenging due to the complicated data transformations inside these models. In this article, we propose Deep Reinforcement Learning Interactive Visual Explorer ( DRLIVE), a visual analytics system to effectively explore, interpret, and diagnose RNN-based DRLs. Having focused on DRL agents trained for different Atari games, DRLIVE accomplishes three tasks: game episode exploration, RNN hidden/cell state examination, and interactive model perturbation. Using the system, one can flexibly explore a DRL agent through interactive visualizations, discover interpretable RNN cells by prioritizing RNN hidden/cell states with a set of metrics, and further diagnose the DRL model by interactively perturbing its inputs. Through concrete studies with multiple deep learning experts, we validated the efficacy of DRLIVE.",
        "author": "Wang, Junpeng and Zhang, Wei and Yang, Hao and Yeh, Chin-Chia Michael and Wang, Liang",
        "doi": "10.1109/TVCG.2021.3076749",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Sequence,data:MD-Array,data:Hybrid,task:Present,task:Explore,task:Compare",
        "number": "12",
        "pages": "4141--4155",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Visual Analytics for RNN-Based Deep Reinforcement Learning",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9420254",
        "volume": "28",
        "year": "2022"
    },
    "wang2022extending": {
        "abstract": "Whether AI explanations can help users achieve specific tasks efficiently (i.e., usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present AI explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network (GNN) predictions to domain experts in drug repurposing, i.e., reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate XAI design considerations from a literature review and from our collaborators' feedback into the design process. Specifically, we discuss XAI-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and XAI goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and XAI and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of DrugExplorer, an XAI tool for drug repurposing. Based on our domain characterization, DrugExplorer provides path-based explanations and presents them both as individual paths and meta-paths for two key XAI operations, why and what else. DrugExplorer offers a novel visualization design called MetaMatrix with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain-meaningful insights. We demonstrate the effectiveness of the selected visual presentation and DrugExplorer as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of XAI visualizations for other scientific applications.",
        "author": "Wang, Qianwen and Huang, Kexin and Chandak, Payal and Zitnik, Marinka and Gehlenborg, Nils",
        "doi": "10.1109/TVCG.2022.3209435",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Graph,task:Present,task:Explore,task:Compare",
        "number": "1",
        "pages": "1266--1276",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9916585",
        "volume": "29",
        "year": "2022"
    },
    "wang2022hetvis": {
        "abstract": "Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concerns. However, due to the security issue and the complexity of deep learning models, it is challenging to investigate data heterogeneity across different clients. To address this issue, based on a requirement analysis we developed a visual analytics tool, HetVis, for participating clients to explore data heterogeneity. We identify data heterogeneity through comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. Combining with the proposed comparison techniques, we develop a novel set of visualizations to identify heterogeneity issues in HFL. We designed three case studies to introduce how HetVis can assist client analysts in understanding different types of heterogeneity issues. Expert reviews and a comparative study demonstrate the effectiveness of HetVis.",
        "author": "Wang, Xumeng and Chen, Wei and Xia, Jiazhi and Wen, Zhen and Zhu, Rongchen and Schreck, Tobias",
        "doi": "10.1109/TVCG.2022.3209347",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9912364",
        "year": "2022"
    },
    "wang2022learning": {
        "abstract": "With the fast-growing number of classification models being produced every day, numerous model interpretation and comparison solutions have also been introduced. For example, LIME and SHAP can interpret what input features contribute more to a classifier's output predictions. Different numerical metrics (e.g., accuracy) can be used to easily compare two classifiers. However, few works can interpret the contribution of a data feature to a classifier in comparison with its contribution to another classifier. This comparative interpretation can help to disclose the fundamental difference between two classifiers, select classifiers in different feature conditions, and better ensemble two classifiers. To accomplish it, we propose a learning-from-disagreement (LFD) framework to visually compare two classification models. Specifically, LFD identifies data instances with disagreed predictions from two compared classifiers and trains a discriminator to learn from the disagreed instances. As the two classifiers' training features may not be available, we train the discriminator through a set of meta-features proposed based on certain hypotheses of the classifiers to probe their behaviors. Interpreting the trained discriminator with the SHAP values of different meta-features, we provide actionable insights into the compared classifiers. Also, we introduce multiple metrics to profile the importance of meta-features from different perspectives. With these metrics, one can easily identify meta-features with the most complementary behaviors in two classifiers, and use them to better ensemble the classifiers. We focus on binary classification models in the financial services and advertising industry to demonstrate the efficacy of our proposed framework and visualizations.",
        "author": "Wang, Junpeng and Wang, Liang and Zheng, Yan and Yeh, Chin-Chia Michael and Jain, Shubham and Zhang, Wei",
        "doi": "10.1109/TVCG.2022.3172107",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Improve",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Learning-From-Disagreement: A Model Comparison and Visual Analytics Framework",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9767606",
        "year": "2023"
    },
    "wexler2019if": {
        "abstract": "A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.",
        "author": "Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and Vi{\\'e}gas, Fernanda and Wilson, Jimbo",
        "doi": "10.1109/TVCG.2019.2934619",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "number": "1",
        "pages": "56--65",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "The What-If Tool: Interactive Probing of Machine Learning Models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber",
        "volume": "26",
        "year": "2019"
    },
    "xenopoulos2022calibrate": {
        "abstract": "Analyzing classification model performance is a crucial task for machine learning practitioners. While practitioners often use count-based metrics derived from confusion matrices, like accuracy, many applications, such as weather prediction, sports betting, or patient risk prediction, rely on a classifier's predicted probabilities rather than predicted labels. In these instances, practitioners are concerned with producing a calibrated model, that is, one which outputs probabilities that reflect those of the true distribution. Model calibration is often analyzed visually, through static reliability diagrams, however, the traditional calibration visualization may suffer from a variety of drawbacks due to the strong aggregations it necessitates. Furthermore, count-based approaches are unable to sufficiently analyze model calibration. We present Calibrate, an interactive reliability diagram that addresses the aforementioned issues. Calibrate constructs a reliability diagram that is resistant to drawbacks in traditional approaches, and allows for interactive subgroup analysis and instance-level inspection. We demonstrate the utility of Calibrate through use cases on both real-world and synthetic data. We further validate Calibrate by presenting the results of a think-aloud experiment with data scientists who routinely analyze model calibration.",
        "author": "Xenopoulos, Peter and Rulff, Joao and Nonato, Luis Gustavo and Barr, Brian and Silva, Claudio",
        "doi": "10.1109/TVCG.2022.3209489",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "853--863",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Calibrate: Interactive Analysis of Probabilistic Model Output",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9904444",
        "volume": "29",
        "year": "2022"
    },
    "xia2021revisiting": {
        "abstract": "Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties. Four controlled experiments were conducted to evaluate how the DR techniques facilitate the tasks of 1) cluster identification, 2) membership identification, 3) distance comparison, and 4) density comparison, respectively. We also evaluated users' subjective preference of the DR techniques regarding the quality of projected clusters. The results show that: 1) Non-linear and Local techniques are preferred in cluster identification and membership identification; 2) Linear techniques perform better than non-linear techniques in density comparison; 3) UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) perform the best in cluster identification and membership identification; 4) NMF (Nonnegative Matrix Factorization) has competitive performance in distance comparison; 5) t-SNLE (t-Distributed Stochastic Neighbor Linear Embedding) has competitive performance in density comparison.",
        "author": "Xia, Jiazhi and Zhang, Yuchen and Song, Jie and Chen, Yang and Wang, Yunhai and Liu, Shixia",
        "doi": "10.1109/TVCG.2021.3114694",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "529--539",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Revisiting dimensionality reduction techniques for visual cluster analysis: an empirical study",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552226",
        "volume": "28",
        "year": "2021"
    },
    "xia2022interactive": {
        "abstract": "We propose a contrastive dimensionality reduction approach (CDR) for interactive visual cluster analysis. Although dimensionality reduction of high-dimensional data is widely used in visual cluster analysis in conjunction with scatterplots, there are several limitations on effective visual cluster analysis. First, it is non-trivial for an embedding to present clear visual cluster separation when keeping neighborhood structures. Second, as cluster analysis is a subjective task, user steering is required. However, it is also non-trivial to enable interactions in dimensionality reduction. To tackle these problems, we introduce contrastive learning into dimensionality reduction for high-quality embedding. We then redefine the gradient of the loss function to the negative pairs to enhance the visual cluster separation of embedding results. Based on the contrastive learning scheme, we employ link-based interactions to steer embeddings. After that, we implement a prototype visual interface that integrates the proposed algorithms and a set of visualizations. Quantitative experiments demonstrate that CDR outperforms existing techniques in terms of preserving correct neighborhood structures and improving visual cluster separation. The ablation experiment demonstrates the effectiveness of gradient redefinition. The user study verifies that CDR outperforms t-SNE and UMAP in the task of cluster identification. We also showcase two use cases on real-world datasets to present the effectiveness of link-based interactions.",
        "author": "Xia, Jiazhi and Huang, Linquan and Lin, Weixing and Zhao, Xin and Wu, Jing and Chen, Yang and Zhao, Ying and Chen, Wei",
        "doi": "10.1109/TVCG.2022.3209423",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,data:Sequence,data:MD-Array,task:Present,task:Compare,task:Assess",
        "number": "1",
        "pages": "734--744",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Interactive visual cluster analysis by contrastive dimensionality reduction",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9904480",
        "volume": "29",
        "year": "2022"
    },
    "xiang2019interactive": {
        "abstract": "In this paper, we develop a visual analysis method for interactively improving the quality of labeled data, which is essential to the success of supervised and semi-supervised learning. The quality improvement is achieved through the use of user-selected trusted items. We employ a bi-level optimization model to accurately match the labels of the trusted items and to minimize the training loss. Based on this model, a scalable data correction algorithm is developed to handle tens of thousands of labeled data efficiently. The selection of the trusted items is facilitated by an incremental tSNE with improved computational efficiency and layout stability to ensure a smooth transition between different levels. We evaluated our method on real-world datasets through quantitative evaluation and case studies, and the results were generally favorable.",
        "author": "Xiang, Shouxing and Ye, Xi and Xia, Jiazhi and Wu, Jing and Chen, Yang and Liu, Shixia",
        "booktitle": "2019 IEEE Conference on Visual Analytics Science and Technology (VAST)",
        "doi": "10.1109/VAST47406.2019.8986943",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Improve",
        "organization": "IEEE",
        "pages": "57--68",
        "series": "VIS (VAST)",
        "title": "Interactive Correction of Mislabeled Training Data",
        "type": "inproceedings",
        "url": "http://orca-mwe.cf.ac.uk/124542/1/Interactive\\_Correction\\_of\\_Mislabeled\\_Training\\_Data.pdf",
        "year": "2019"
    },
    "xie2021fairrankvis": {
        "abstract": "Graph mining is an essential component of recommender systems and search engines. Outputs of graph mining models typically provide a ranked list sorted by each item's relevance or utility. However, recent research has identified issues of algorithmic bias in such models, and new graph mining algorithms have been proposed to correct for bias. As such, algorithm developers need tools that can help them uncover potential biases in their models while also exploring the impacts of correcting for biases when employing fairness-aware algorithms. In this paper, we present FairRankVis, a visual analytics framework designed to enable the exploration of multi-class bias in graph mining algorithms. We support both group and individual fairness levels of comparison. Our framework is designed to enable model developers to compare multi-class fairness between algorithms (for example, comparing PageRank with a debiased PageRank algorithm) to assess the impacts of algorithmic debiasing with respect to group and individual fairness. We demonstrate our framework through two usage scenarios inspecting algorithmic fairness.",
        "author": "Xie, Tiankai and Ma, Yuxin and Kang, Jian and Tong, Hanghang and Maciejewski, Ross",
        "doi": "10.1109/TVCG.2021.3114850",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Graph,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "368--377",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552229",
        "volume": "28",
        "year": "2021"
    },
    "xuan2022vac": {
        "abstract": "The rapid development of Convolutional Neural Networks (CNNs) in recent years has triggered significant breakthroughs in many machine learning (ML) applications. The ability to understand and compare various CNN models available is thus essential. The conventional approach with visualizing each model's quantitative features, such as classification accuracy and computational complexity, is not sufficient for a deeper understanding and comparison of the behaviors of different models. Moreover, most of the existing tools for assessing CNN behaviors only support comparison between two models and lack the flexibility of customizing the analysis tasks according to user needs. This paper presents a visual analytics system, VAC-CNN ( V isual A nalytics for C omparing CNNs ), that supports the in-depth inspection of a single CNN model as well as comparative studies of two or more models. The ability to compare a larger number of (e.g., tens of) models especially distinguishes our system from previous ones. With a carefully designed model visualization and explaining support, VAC-CNN facilitates a highly interactive workflow that promptly presents both quantitative and qualitative information at each analysis stage. We demonstrate VAC-CNN's effectiveness for assisting novice ML practitioners in evaluating and comparing multiple CNN models through two use cases and one preliminary evaluation study using the image classification tasks on the ImageNet dataset.",
        "author": "Xuan, Xiwei and Zhang, Xiaoyu and Kwon, Oh-Hyun and Ma, Kwan-Liu",
        "doi": "10.1109/TVCG.2022.3165347",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "6",
        "pages": "2326--2337",
        "publisher": "IEEE",
        "series": "PacificVis",
        "title": "VAC-CNN: A Visual Analytics System for Comparative Studies of Deep Convolutional Neural Networks",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9751204",
        "volume": "28",
        "year": "2022"
    },
    "yang2020diagnosing": {
        "abstract": "Concept drift is a phenomenon in which the distribution of a data stream changes over time in unforeseen ways, causing prediction models built on historical data to become inaccurate. While a variety of automated methods have been developed to identify when concept drift occurs, there is limited support for analysts who need to understand and correct their models when drift is detected. In this paper, we present a visual analytics method, DriftVis, to support model builders and analysts in the identification and correction of concept drift in streaming data. DriftVis combines a distribution-based drift detection method with a streaming scatterplot to support the analysis of drift caused by the distribution changes of data streams and to explore the impact of these changes on the model\u2019s accuracy. A quantitative experiment and two case studies on weather prediction and text classification have been conducted to demonstrate our proposed tool and illustrate how visual analytics can be used to support the detection, examination, and correction of concept drift.",
        "author": "Yang, Weikai and Li, Zhen and Liu, Mengchen and Lu, Yafeng and Cao, Kelei and Maciejewski, Ross and Liu, Shixia",
        "booktitle": "2020 IEEE Conference on Visual Analytics Science and Technology (VAST)",
        "doi": "10.1109/VAST50239.2020.00007",
        "keywords": "type:General ML,data:Sequence,task:Present,task:Explore,task:Compare,task:Assess",
        "pages": "12-23",
        "series": "VIS (VAST)",
        "title": "Diagnosing concept drift with visual analytics",
        "type": "inproceedings",
        "url": "https://arxiv.org/pdf/2007.14372.pdf",
        "year": "2020"
    },
    "yang2020interactive": {
        "abstract": "Hierarchical clustering is an important technique to organize big data for exploratory data analysis. However, existing one-size-fits-all hierarchical clustering methods often fail to meet the diverse needs of different users. To address this challenge, we present an interactive steering method to visually supervise constrained hierarchical clustering by utilizing both public knowledge (e.g., Wikipedia) and private knowledge from users. The novelty of our approach includes 1) automatically constructing constraints for hierarchical clustering using knowledge (knowledge-driven) and intrinsic data distribution (data-driven), and 2) enabling the interactive steering of clustering through a visual interface (user-driven). Our method first maps each data item to the most relevant items in a knowledge base. An initial constraint tree is then extracted using the ant colony optimization algorithm. The algorithm balances the tree width and depth and covers the data items with high confidence. Given the constraint tree, the data items are hierarchically clustered using evolutionary Bayesian rose tree. To clearly convey the hierarchical clustering results, an uncertainty-aware tree visualization has been developed to enable users to quickly locate the most uncertain sub-hierarchies and interactively improve them. The quantitative evaluation and case study demonstrate that the proposed approach facilitates the building of customized clustering trees in an efficient and effective manner.",
        "author": "Yang, Weikai and Wang, Xiting and Lu, Jie and Dou, Wenwen and Liu, Shixia",
        "doi": "https://ieeexplore.ieee.org/abstract/document/9094378",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Sequence,data:Graph,task:Present,task:Explore,task:Assess,task:Improve",
        "number": "10",
        "pages": "3953--3967",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Interactive steering of hierarchical clustering",
        "type": "article",
        "url": "10.1109/TVCG.2020.2995100",
        "volume": "27",
        "year": "2020"
    },
    "yang2022diagnosing": {
        "abstract": "The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12\\% and 21\\%, respectively.",
        "author": "Yang, Weikai and Ye, Xi and Zhang, Xingxing and Xiao, Lanxi and Xia, Jiazhi and Wang, Zhongyuan and Zhu, Jun and Pfister, Hanspeter and Liu, Shixia",
        "doi": "10.1109/TVCG.2022.3182488",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Improve",
        "number": "9",
        "pages": "3292--3306",
        "publisher": "IEEE",
        "series": "TVCG",
        "title": "Diagnosing Ensemble Few-Shot Classifiers",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/9795241",
        "volume": "28",
        "year": "2022"
    },
    "ye2019interactive": {
        "abstract": "We present a novel interactive learning-based method for curating datasets using user-defined criteria for training and refining Generative Adversarial Networks. We employ a novel batch-mode active learning strategy to progressively select small batches of candidate exemplars for which the user is asked to indicate whether they match the, possibly subjective, selection criteria. After each batch, a classifier that models the user's intent is refined and subsequently used to select the next batch of candidates. After the selection process ends, the final classifier, trained with limited but adaptively selected training data, is used to sift through the large collection of input exemplars to extract a sufficiently large subset for training or refining the generative model that matches the user's selection criteria. A key distinguishing feature of our system is that we do not assume that the user can always make a firm binary decision (i.e., \u201cmeets\u201d or \u201cdoes not meet\u201d the selection criteria) for each candidate exemplar, and we allow the user to label an exemplar as \u201cundecided\u201d. We rely on a non-binary query-by-committee strategy to distinguish between the user's uncertainty and the trained classifier's uncertainty, and develop a novel disagreement distance metric to encourage a diverse candidate set. In addition, a number of optimization strategies are employed to achieve an interactive experience. We demonstrate our interactive curation system on several applications related to training or refining generative models: training a Generative Adversarial Network that meets a user-defined criteria, adjusting the output distribution of an existing generative model, and removing unwanted samples from a generative model.",
        "author": "Ye, Wenjie and Dong, Yue and Peers, Pieter",
        "booktitle": "Computer Graphics Forum",
        "doi": "https://doi.org/10.1111/cgf.13844",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Improve",
        "number": "7",
        "organization": "Wiley Online Library",
        "pages": "369--380",
        "series": "CGF",
        "title": "Interactive curation of datasets for training and refining generative models",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.13844",
        "volume": "38",
        "year": "2019"
    },
    "yuan2022subplex": {
        "abstract": "Understanding the interpretation of machine learning (ML) models has been of paramount importance when making decisions with societal impacts, such as transport control, financial activities, and medical diagnosis. While local explanation techniques are popular methods to interpret ML models on a single instance, they do not scale to the understanding of a model\u2019s behavior on the whole dataset. In this article, we outline the challenges and needs of visually analyzing local explanations and propose SUBPLEX , a visual analytics approach to help users understand local explanations with subpopulation visual analysis. SUBPLEX provides steerable clustering and projection visualization techniques that allow users to derive interpretable subpopulations of local explanations with users\u2019 expertise. We evaluate our approach through two use cases and experts' feedback.",
        "author": "Yuan, Jun and Chan, Gromit Yeuk-Yin and Barr, Brian and Overton, Kyle and Rees, Kim and Nonato, Luis Gustavo and Bertini, Enrico and Silva, Claudio T",
        "doi": "10.1109/MCG.2022.3199727",
        "journal": "IEEE Computer Graphics and Applications",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare",
        "number": "6",
        "pages": "24--36",
        "publisher": "IEEE",
        "series": "CG\\&A",
        "title": "SUBPLEX: A Visual Analytics Approach to Understand Local Model Explanations At the Subpopulation Level",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9861728",
        "volume": "42",
        "year": "2022"
    },
    "zhang2016visual": {
        "abstract": "In the domain of epidemiology, logistic regression modeling is widely used to explain the relationships among explanatory variables and dichotomous outcome variables. However, logistic regression modeling faces challenges such as overfitting, confounding, and multicollinearity when there is a large number of explanatory variables. For example, in the birth defect study presented in this paper, variable selection for building high quality models to identify risk factors from hundreds of pollutant variables is difficult. To address this problem, we propose a novel visual analytics approach to logistic regression modeling for high-dimensional datasets. It leverages the traditional modeling pipeline by providing (1) intuitive visualizations for inspecting statistical indicators and the relationships among the variables and (2) a seamless, effective dimension reduction pipeline for selecting variables for inclusion in high quality logistic regression models. A fully working prototype of this approach has been developed and successfully applied to the birth defect study, which illustrates its effectiveness and efficiency. Its application in an insurance policy study and feedback from domain experts further demonstrate its usefulness.",
        "author": "Zhang, Chong and Yang, Jing and Zhan, F Benjamin and Gong, Xi and Brender, Jean D and Langlois, Peter H and Barlowe, Scott and Zhao, Ye",
        "booktitle": "2016 IEEE Pacific Visualization Symposium (PacificVis)",
        "doi": "10.1109/PACIFICVIS.2016.7465261",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess,task:Improve",
        "pages": "136--143",
        "publisher": "IEEE",
        "series": "PacificVis",
        "title": "A visual analytics approach to high-dimensional logistic regression modeling and its application to an environmental health study",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/7465261",
        "year": "2016"
    },
    "zhang2018manifold": {
        "abstract": "Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.",
        "author": "Zhang, Jiawei and Wang, Yang and Molino, Piero and Li, Lezhi and Ebert, David S",
        "doi": "10.1109/TVCG.2018.2864499",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "1",
        "pages": "364--373",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models",
        "type": "article",
        "url": "https://arxiv.org/pdf/1808.00196.pdf",
        "volume": "25",
        "year": "2018"
    },
    "zhang2022sliceteller": {
        "abstract": "Real-world machine learning applications need to be thoroughly evaluated to meet critical product requirements for model release, to ensure fairness for different groups or individuals, and to achieve a consistent performance in various scenarios. For example, in autonomous driving, an object classification model should achieve high detection rates under different conditions of weather, distance, etc. Similarly, in the financial setting, credit-scoring models must not discriminate against minority groups. These conditions or groups are called as \u201c Data Slices \u201d. In product MLOps cycles, product developers must identify such critical data slices and adapt models to mitigate data slice problems. Discovering where models fail, understanding why they fail, and mitigating these problems, are therefore essential tasks in the MLOps life-cycle. In this paper, we present SliceTeller , a novel tool that allows users to debug, compare and improve machine learning models driven by critical data slices. SliceTeller automatically discovers problematic slices in the data, helps the user understand why models fail. More importantly, we present an efficient algorithm, SliceBoosting , to estimate trade-offs when prioritizing the optimization over certain slices. Furthermore, our system empowers model developers to compare and analyze different model versions during model iterations, allowing them to choose the model version best suitable for their applications. We evaluate our system with three use cases, including two real-world use cases of product development , to demonstrate the power of SliceTeller in the debugging and improvement of product-quality ML models.",
        "author": "Zhang, Xiaoyu and Ono, Jorge Piazentin and Song, Huan and Gou, Liang and Ma, Kwan-Liu and Ren, Liu",
        "doi": "10.1109/TVCG.2022.3209465",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:Tabular,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Improve",
        "number": "1",
        "pages": "842--852",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "SliceTeller: A Data Slice-Driven Approach for Machine Learning Model Validation",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9906903",
        "volume": "29",
        "year": "2022"
    },
    "zhao2018iforest": {
        "abstract": "As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.",
        "author": "Zhao, Xun and Wu, Yanhong and Lee, Dik Lun and Cui, Weiwei",
        "journal": "IEEE transactions on visualization and computer graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Assess",
        "number": "1",
        "pages": "407--416",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "iforest: Interpreting random forests via visual analytics",
        "type": "article",
        "volume": "25",
        "year": "2018"
    },
    "zhao2019featureexplorer": {
        "abstract": "Feature selection is used in machine learning to improve predictions, decrease computation time, reduce noise, and tune models based on limited sample data. In this article, we present FeatureExplorer, a visual analytics system that supports the dynamic evaluation of regression models and importance of feature subsets through the interactive selection of features in high-dimensional feature spaces typical of hyperspectral images. The interactive system allows users to iteratively refine and diagnose the model by selecting features based on their domain knowledge, interchangeable (correlated) features, feature importance, and the resulting model performance.",
        "author": "Zhao, Jieqiong and Karimzadeh, Morteza and Masjedi, Ali and Wang, Taojun and Zhang, Xiwen and Crawford, Melba M and Ebert, David S",
        "booktitle": "2019 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VISUAL.2019.8933619",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "organization": "IEEE",
        "pages": "161--165",
        "series": "VIS Short",
        "title": "FeatureExplorer: Interactive feature selection and exploration of regression models for hyperspectral images",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/8933619",
        "year": "2019"
    },
    "zhao2019oui": {
        "abstract": "Outliers, the data instances that do not conform with normal patterns in a dataset, are widely studied in various domains, such as cybersecurity, social analysis, and public health. By detecting and analyzing outliers, users can either gain insights into abnormal patterns or purge the data of errors. However, different domains usually have different considerations with respect to outliers. Understanding the defining characteristics of outliers is essential for users to select and filter appropriate outliers based on their domain requirements. Unfortunately, most existing work focuses on the efficiency and accuracy of outlier detection, neglecting the importance of outlier interpretation. To address these issues, we propose Oui, a visual analytic system that helps users understand, interpret, and select the outliers detected by various algorithms. We also present a usage scenario on a real dataset and a qualitative user study to demonstrate the effectiveness and usefulness of our system.",
        "author": "Zhao, Xun and Cui, Weiwei and Wu, Yanhong and Zhang, Haidong and Qu, Huamin and Zhang, Dongmei",
        "booktitle": "Computer Graphics Forum",
        "doi": "https://doi.org/10.1111/cgf.13683",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare,task:Assess",
        "number": "3",
        "organization": "Wiley Online Library",
        "pages": "213--224",
        "series": "EuroVis",
        "title": "Oui! Outlier Interpretation on Multi-dimensional Data via Visual Analytics",
        "type": "inproceedings",
        "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13683",
        "volume": "38",
        "year": "2019"
    },
    "zhao2020protoviewer": {
        "abstract": "In recent years deep neural networks (DNNs) are increasingly used in a variety of application domains for their state-of-the-art performance in many challenging machine learning tasks. However their lack of interpretability could cause trustability and fairness issues and also makes model diagnostics a difficult task. In this paper we present a novel visual analytics framework to interpret and diagnose DNNs. Our approach utilizes ProtoFac to factorize the latent representations in DNNs into weighted combinations of prototypes, which are exemplar cases (e.g., representative image patches) from the original data. The visual interface uses the factorized prototypes to summarize and explain the model behaviour as well as support comparisons across subsets of data such that the users can form a hypothesis about the model's failure on certain subsets. The method is model-agnostic and provides global explanation of the model behaviour. Furthermore, the system selects prototypes and weights that faithfully represents the model under analysis by mimicking its latent representation and predictions. Example usage scenarios on two DNN architectures and two datasets illustrates the effectiveness and general applicability of the proposed approach.",
        "author": "Zhao, Junhan and Dai, Zeng and Xu, Panpan and Ren, Liu",
        "booktitle": "2020 IEEE Visualization Conference (VIS)",
        "doi": "10.1109/VIS47514.2020.00064",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Assess",
        "pages": "286--290",
        "series": "Vis Short",
        "title": "ProtoViewer: Visual Interpretation and Diagnostics of Deep Neural Networks with Factorized Prototypes",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/abstract/document/9331285",
        "year": "2020"
    },
    "zhao2021human": {
        "abstract": "The interpretation of deep neural networks (DNNs) has become a key topic as more and more people apply them to solve various problems and making critical decisions. Concept-based explanations have recently become a popular approach for post-hoc interpretation of DNNs. However, identifying human-understandable visual concepts that affect model decisions is a challenging task that is not easily addressed with automatic approaches. We present a novel human-in-the-Ioop approach to generate user-defined concepts for model interpretation and diagnostics. Central to our proposal is the use of active learning, where human knowledge and feedback are combined to train a concept extractor with very little human labeling effort. We integrate this process into an interactive system, ConceptExtract. Through two case studies, we show how our approach helps analyze model behavior and extract human-friendly concepts for different machine learning tasks and datasets and how to use these concepts to understand the predictions, compare model performance and make suggestions for model refinement. Quantitative experiments show that our active learning approach can accurately extract meaningful visual concepts. More importantly, by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance.",
        "author": "Zhao, Zhenge and Xu, Panpan and Scheidegger, Carlos and Ren, Liu",
        "doi": "10.1109/TVCG.2021.3114837",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:DL,data:MD-Array,task:Present,task:Explore,task:Compare,task:Assess,task:Generate",
        "number": "1",
        "pages": "780--790",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Human-in-the-loop extraction of interpretable concepts in deep learning models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552218",
        "volume": "28",
        "year": "2021"
    },
    "zytek2021sibyl": {
        "abstract": "Machine learning (ML) is being applied to a diverse and ever-growing set of domains. In many cases, domain experts - who often have no expertise in ML or data science - are asked to use ML predictions to make high-stakes decisions. Multiple ML usability challenges can appear as result, such as lack of user trust in the model, inability to reconcile human-ML disagreement, and ethical concerns about oversimplification of complex problems to a single algorithm output. In this paper, we investigate the ML usability challenges that present in the domain of child welfare screening through a series of collaborations with child welfare screeners. Following the iterative design process between the ML scientists, visualization researchers, and domain experts (child screeners), we first identified four key ML challenges and honed in on one promising explainable ML technique to address them (local factor contributions). Then we implemented and evaluated our visual analytics tool, Sibyl, to increase the interpretability and interactivity of local factor contributions. The effectiveness of our tool is demonstrated by two formal user studies with 12 non-expert participants and 13 expert participants respectively. Valuable feedback was collected, from which we composed a list of design implications as a useful guideline for researchers who aim to develop an interpretable and interactive visualization tool for ML prediction models deployed for child welfare screeners and other similar domain experts.",
        "author": "Zytek, Alexandra and Liu, Dongyu and Vaithianathan, Rhema and Veeramachaneni, Kalyan",
        "doi": "10.1109/TVCG.2021.3114864",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "keywords": "type:General ML,data:Tabular,task:Present,task:Explore,task:Compare",
        "number": "1",
        "pages": "1161--1171",
        "publisher": "IEEE",
        "series": "VIS",
        "title": "Sibyl: Understanding and addressing the usability challenges of machine learning in high-stakes decision making",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/abstract/document/9552849",
        "volume": "28",
        "year": "2021"
    }
};